{
  "filename": "experimental_004.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:46:05.661773",
  "token_count": 330,
  "perplexity": {
    "overall": 1.7336,
    "per_token": [
      2.578,
      4.1849,
      1.1649,
      4.6843,
      3.0279,
      2.4705,
      1.0005,
      1.2749,
      2.1697,
      5.1186,
      1.6542,
      2.0324,
      1.0,
      1.0645,
      2.185,
      3.7034,
      1.0197,
      2.4396,
      1.835,
      1.071,
      1.2693,
      1.1179,
      1.0,
      1.0029,
      1.2414,
      2.5645,
      1.0157,
      1.6267,
      2.1396,
      1.0444,
      1.2585,
      1.1122,
      1.0,
      1.0051,
      1.3541,
      1.4743,
      1.0092,
      1.7029,
      2.2589,
      1.0421,
      2.0754,
      7.4289,
      1.4443,
      2.3552,
      2.0071,
      2.4491,
      2.282,
      1.157,
      1.0122,
      1.0106,
      2.1995,
      6.4389,
      1.0665,
      1.405,
      1.0342,
      1.6324,
      1.0002,
      1.0439,
      4.5021,
      5.3972,
      1.9766,
      2.5502,
      4.7962,
      1.242,
      1.0001,
      1.0505,
      10.1123,
      2.619,
      1.0225,
      1.018,
      2.0849,
      4.0597,
      1.7639,
      1.2865,
      1.0002,
      1.2563,
      1.1698,
      3.5183,
      1.1637,
      3.2678,
      1.797,
      4.3535,
      1.1063,
      1.4445,
      1.0341,
      1.4922,
      1.0002,
      1.0837,
      1.5693,
      2.2499,
      2.2341,
      4.5101,
      1.3582,
      1.334,
      1.0571,
      1.3341,
      1.0002,
      1.1942,
      1.2771,
      3.5786,
      1.4273,
      3.9953,
      3.0047,
      1.1724,
      1.0,
      1.1229,
      10.767,
      1.0308,
      4.6063,
      1.0039,
      2.1616,
      6.3111,
      1.4292,
      9.6612,
      1.9423,
      7.4423,
      4.0784,
      1.6199,
      1.0865,
      1.7264,
      2.2526,
      4.3209,
      2.0635,
      1.335,
      1.0001,
      1.1259,
      1.4585,
      4.5359,
      1.2508,
      3.5065,
      1.3362,
      4.6056,
      1.1924,
      1.4635,
      1.0383,
      1.579,
      1.0002,
      1.0884,
      1.9057,
      2.6996,
      1.5366,
      4.1963,
      1.1246,
      1.8823,
      1.0412,
      1.3714,
      1.0001,
      1.2102,
      1.4915,
      2.1792,
      2.399,
      2.9763,
      4.3647,
      1.3829,
      1.0001,
      1.2483,
      1.2664,
      5.0107,
      1.0006,
      1.7052,
      1.6914,
      4.709,
      1.1336,
      1.3467,
      1.0527,
      1.238,
      1.0001,
      1.187,
      1.3316,
      3.6573,
      2.9844,
      4.0056,
      1.6071,
      2.3453,
      2.0343,
      1.8483,
      1.8723,
      1.1135,
      1.0166,
      1.0071,
      2.3396,
      6.4828,
      1.3784,
      2.1062,
      1.9731,
      2.0608,
      2.2017,
      1.1254,
      1.0077,
      1.0094,
      1.8216,
      5.3983,
      2.2481,
      1.3475,
      1.0002,
      1.199,
      1.3421,
      3.3334,
      1.1421,
      4.6521,
      2.3922,
      1.2709,
      1.0,
      1.0111,
      2.1519,
      1.7941,
      1.1122,
      2.8267,
      1.1107,
      1.1059,
      2.1352,
      4.2581,
      2.7594,
      1.337,
      1.0002,
      1.2502,
      1.2032,
      4.9381,
      1.0004,
      1.4991,
      1.8599,
      2.8058,
      3.4677,
      1.2219,
      1.0001,
      1.1345,
      1.3777,
      3.3012,
      1.0005,
      1.616,
      1.4935,
      4.568,
      1.0579,
      1.6956,
      1.0505,
      1.3506,
      1.0001,
      1.1171,
      1.4902,
      4.6175,
      1.5798,
      1.3054,
      1.0,
      1.0164,
      2.2277,
      1.8033,
      1.1614,
      3.0212,
      1.2702,
      1.2085,
      2.1816,
      4.3452,
      4.3308,
      1.4717,
      1.0003,
      1.2901,
      1.1423,
      2.6282,
      1.0003,
      3.39,
      1.6132,
      5.4721,
      1.0894,
      1.7241,
      1.1065,
      1.2681,
      1.0001,
      1.0626,
      1.4918,
      3.0029,
      2.241,
      3.8929,
      1.1293,
      1.5531,
      1.1877,
      1.4572,
      1.0006,
      1.0955,
      1.2747,
      2.1354,
      1.6214,
      3.0221,
      1.1032,
      1.6286,
      1.0881,
      1.456,
      1.0006,
      1.138,
      1.2651,
      2.2682,
      2.3715,
      3.8591,
      2.8136,
      1.4117,
      1.0002,
      1.2161,
      1.2262,
      5.0341,
      1.1708,
      5.373,
      2.1516,
      5.1878,
      2.1498,
      1.4041,
      1.0008,
      1.2823,
      1.2833,
      3.8394,
      1.0007,
      1.993,
      1.7645,
      2.3771,
      1.1193,
      1.4681,
      1.0993,
      1.3072,
      1.0002,
      1.1831,
      1.3176,
      2.7666,
      1.2771,
      2.0147,
      1.0275,
      1.3605,
      1.6543,
      2.5672,
      1.0477,
      1.761,
      1.1406,
      1.6847
    ],
    "max": 10.767,
    "min": 1.0,
    "mean": 2.0569
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7937,
    "per_token": [
      1.3662,
      2.0652,
      0.2202,
      2.2278,
      1.5983,
      1.3048,
      0.0007,
      0.3504,
      1.1175,
      2.3558,
      0.7262,
      1.0232,
      0.0,
      0.0902,
      1.1276,
      1.8888,
      0.0281,
      1.2866,
      0.8758,
      0.099,
      0.3441,
      0.1608,
      0.0,
      0.0042,
      0.312,
      1.3587,
      0.0225,
      0.702,
      1.0973,
      0.0627,
      0.3317,
      0.1534,
      0.0,
      0.0074,
      0.4373,
      0.56,
      0.0132,
      0.768,
      1.1756,
      0.0595,
      1.0534,
      2.8932,
      0.5304,
      1.2358,
      1.0051,
      1.2923,
      1.1903,
      0.2104,
      0.0175,
      0.0152,
      1.1372,
      2.6868,
      0.0928,
      0.4906,
      0.0486,
      0.707,
      0.0003,
      0.062,
      2.1706,
      2.4322,
      0.983,
      1.3506,
      2.2619,
      0.3127,
      0.0002,
      0.071,
      3.338,
      1.389,
      0.0321,
      0.0257,
      1.06,
      2.0214,
      0.8188,
      0.3635,
      0.0003,
      0.3292,
      0.2263,
      1.8149,
      0.2187,
      1.7083,
      0.8456,
      2.1222,
      0.1458,
      0.5306,
      0.0483,
      0.5775,
      0.0002,
      0.1159,
      0.6501,
      1.1699,
      1.1597,
      2.1732,
      0.4417,
      0.4157,
      0.0801,
      0.4158,
      0.0002,
      0.2561,
      0.3529,
      1.8394,
      0.5133,
      1.9983,
      1.5872,
      0.2295,
      0.0001,
      0.1673,
      3.4285,
      0.0438,
      2.2036,
      0.0056,
      1.1121,
      2.6579,
      0.5152,
      3.2722,
      0.9577,
      2.8957,
      2.028,
      0.6959,
      0.1197,
      0.7878,
      1.1716,
      2.1113,
      1.0451,
      0.4168,
      0.0001,
      0.1711,
      0.5445,
      2.1814,
      0.3228,
      1.81,
      0.4182,
      2.2034,
      0.2538,
      0.5494,
      0.0542,
      0.659,
      0.0003,
      0.1222,
      0.9303,
      1.4328,
      0.6197,
      2.0691,
      0.1695,
      0.9125,
      0.0583,
      0.4557,
      0.0002,
      0.2753,
      0.5767,
      1.1238,
      1.2624,
      1.5735,
      2.1259,
      0.4677,
      0.0002,
      0.3199,
      0.3407,
      2.325,
      0.0009,
      0.77,
      0.7582,
      2.2354,
      0.1809,
      0.4294,
      0.0741,
      0.308,
      0.0001,
      0.2473,
      0.4132,
      1.8708,
      1.5774,
      2.002,
      0.6845,
      1.2298,
      1.0245,
      0.8862,
      0.9048,
      0.1551,
      0.0238,
      0.0103,
      1.2263,
      2.6966,
      0.463,
      1.0746,
      0.9804,
      1.0432,
      1.1386,
      0.1704,
      0.0111,
      0.0135,
      0.8652,
      2.4325,
      1.1687,
      0.4303,
      0.0004,
      0.2619,
      0.4245,
      1.737,
      0.1917,
      2.2179,
      1.2584,
      0.3458,
      0.0,
      0.016,
      1.1056,
      0.8432,
      0.1534,
      1.4991,
      0.1515,
      0.1452,
      1.0944,
      2.0902,
      1.4643,
      0.419,
      0.0003,
      0.3221,
      0.2669,
      2.304,
      0.0006,
      0.5841,
      0.8952,
      1.4884,
      1.794,
      0.2892,
      0.0002,
      0.182,
      0.4623,
      1.723,
      0.0007,
      0.6925,
      0.5787,
      2.1916,
      0.0812,
      0.7618,
      0.0711,
      0.4336,
      0.0001,
      0.1598,
      0.5755,
      2.2071,
      0.6597,
      0.3845,
      0.0,
      0.0235,
      1.1556,
      0.8507,
      0.2159,
      1.5951,
      0.3451,
      0.2732,
      1.1254,
      2.1194,
      2.1146,
      0.5575,
      0.0004,
      0.3675,
      0.1919,
      1.3941,
      0.0004,
      1.7613,
      0.6899,
      2.4521,
      0.1236,
      0.7859,
      0.146,
      0.3427,
      0.0001,
      0.0876,
      0.577,
      1.5863,
      1.1641,
      1.9608,
      0.1754,
      0.6351,
      0.2482,
      0.5432,
      0.0008,
      0.1316,
      0.3501,
      1.0945,
      0.6972,
      1.5955,
      0.1416,
      0.7037,
      0.1219,
      0.542,
      0.0008,
      0.1865,
      0.3393,
      1.1815,
      1.2458,
      1.9483,
      1.4924,
      0.4975,
      0.0004,
      0.2823,
      0.2942,
      2.3317,
      0.2275,
      2.4257,
      1.1054,
      2.3751,
      1.1042,
      0.4897,
      0.0011,
      0.3587,
      0.3598,
      1.9409,
      0.0009,
      0.995,
      0.8192,
      1.2492,
      0.1626,
      0.5539,
      0.1366,
      0.3865,
      0.0003,
      0.2426,
      0.3979,
      1.4681,
      0.3529,
      1.0105,
      0.0391,
      0.4442,
      0.7262,
      1.3602,
      0.0672,
      0.8164,
      0.1898,
      0.7525
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.3989,
    "entropy_zscore": 0.4455,
    "surprisal_zscore": 0.4455,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}