{
  "filename": "noise_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:00.116796",
  "token_count": 120,
  "perplexity": {
    "overall": 1.7378,
    "per_token": [
      3.604,
      1.8331,
      1.0,
      1.0613,
      2.0717,
      4.408,
      1.3524,
      3.1134,
      5.4521,
      1.9593,
      2.1014,
      2.0113,
      1.0,
      1.0532,
      2.2372,
      2.2089,
      1.5239,
      1.4873,
      2.2365,
      2.7194,
      1.4811,
      2.0382,
      1.0392,
      1.0442,
      1.1802,
      2.3526,
      1.5787,
      2.5668,
      1.0131,
      1.0072,
      2.6636,
      1.9374,
      1.0,
      1.2049,
      1.7927,
      2.7132,
      1.1028,
      2.4019,
      1.3162,
      1.64,
      2.8431,
      5.7638,
      1.9907,
      1.2747,
      1.0002,
      1.1946,
      1.3286,
      4.5268,
      1.1558,
      6.597,
      2.0777,
      2.0044,
      1.0206,
      1.5206,
      1.6054,
      3.2277,
      1.0603,
      1.8637,
      1.0687,
      1.254,
      2.2457,
      1.7948,
      1.0,
      1.0808,
      1.6484,
      3.1745,
      1.0897,
      1.6515,
      1.1862,
      1.4545,
      2.2778,
      4.72,
      2.5235,
      1.2196,
      1.0002,
      1.2024,
      1.2317,
      4.3862,
      1.0817,
      4.0394,
      1.6464,
      4.1692,
      1.1236,
      1.4703,
      1.1039,
      1.7197,
      1.0004,
      1.1128,
      1.2745,
      4.6151,
      2.022,
      5.6881,
      1.1559,
      1.5521,
      1.0983,
      1.4722,
      1.0011,
      1.0805,
      1.2416,
      2.0262,
      1.8706,
      3.4685,
      2.9588,
      1.2449,
      1.0001,
      1.1225,
      9.6582,
      1.0435,
      1.2933,
      2.2659,
      1.6319,
      4.6364,
      1.2337,
      1.3954,
      1.0819,
      1.7949,
      1.0007,
      1.1464,
      1.2738,
      2.2028
    ],
    "max": 9.6582,
    "min": 1.0,
    "mean": 2.0233
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7972,
    "per_token": [
      1.8496,
      0.8743,
      0.0,
      0.0859,
      1.0508,
      2.1401,
      0.4355,
      1.6385,
      2.4468,
      0.9703,
      1.0713,
      1.0081,
      0.0,
      0.0748,
      1.1617,
      1.1434,
      0.6077,
      0.5727,
      1.1613,
      1.4433,
      0.5667,
      1.0273,
      0.0555,
      0.0624,
      0.2391,
      1.2343,
      0.6587,
      1.36,
      0.0188,
      0.0103,
      1.4134,
      0.9541,
      0.0,
      0.2689,
      0.8422,
      1.44,
      0.1412,
      1.2642,
      0.3963,
      0.7137,
      1.5075,
      2.527,
      0.9933,
      0.3502,
      0.0003,
      0.2565,
      0.4099,
      2.1785,
      0.2089,
      2.7218,
      1.055,
      1.0032,
      0.0294,
      0.6046,
      0.6829,
      1.6905,
      0.0845,
      0.8982,
      0.0958,
      0.3266,
      1.1672,
      0.8438,
      0.0,
      0.1121,
      0.7211,
      1.6665,
      0.1239,
      0.7238,
      0.2464,
      0.5405,
      1.1877,
      2.2388,
      1.3354,
      0.2865,
      0.0003,
      0.2659,
      0.3007,
      2.133,
      0.1133,
      2.0142,
      0.7194,
      2.0598,
      0.1682,
      0.5561,
      0.1427,
      0.7822,
      0.0006,
      0.1542,
      0.35,
      2.2063,
      1.0158,
      2.5079,
      0.209,
      0.6342,
      0.1353,
      0.558,
      0.0015,
      0.1117,
      0.3122,
      1.0188,
      0.9035,
      1.7943,
      1.565,
      0.3161,
      0.0001,
      0.1668,
      3.2718,
      0.0615,
      0.371,
      1.1801,
      0.7066,
      2.213,
      0.303,
      0.4807,
      0.1136,
      0.8439,
      0.001,
      0.1971,
      0.3491,
      1.1394
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.4302,
    "entropy_zscore": 0.4768,
    "surprisal_zscore": 0.4768,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}