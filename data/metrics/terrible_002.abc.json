{
  "filename": "terrible_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:22.865819",
  "token_count": 70,
  "perplexity": {
    "overall": 1.6023,
    "per_token": [
      1.3913,
      1.7448,
      1.0,
      1.0459,
      1.4605,
      1.5938,
      1.8275,
      4.1866,
      3.7502,
      2.3264,
      1.8105,
      1.4798,
      1.0,
      1.0099,
      1.3741,
      1.894,
      1.0097,
      1.9772,
      2.5367,
      1.0017,
      1.4179,
      4.5685,
      1.3502,
      2.0219,
      2.473,
      1.2379,
      3.6775,
      1.1823,
      1.0066,
      1.0233,
      1.6152,
      1.3126,
      1.3828,
      1.4053,
      1.0349,
      1.4238,
      1.0002,
      1.1214,
      1.1838,
      1.2685,
      1.3512,
      5.9975,
      1.2229,
      3.9123,
      4.0284,
      8.1212,
      1.5791,
      1.1306,
      1.1425,
      2.2868,
      2.3453,
      1.1472,
      1.0,
      1.0071,
      2.0377,
      3.5321,
      1.0012,
      1.49,
      1.1577,
      1.0097,
      1.2101,
      1.1168,
      1.0,
      1.0332,
      2.1798,
      2.1199,
      1.0034,
      1.7995,
      2.5285,
      1.0377
    ],
    "max": 8.1212,
    "min": 1.0,
    "mean": 1.8523
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.6802,
    "per_token": [
      0.4765,
      0.8031,
      0.0,
      0.0647,
      0.5465,
      0.6725,
      0.8699,
      2.0658,
      1.907,
      1.2181,
      0.8564,
      0.5654,
      0.0,
      0.0142,
      0.4585,
      0.9215,
      0.014,
      0.9835,
      1.3429,
      0.0024,
      0.5037,
      2.1917,
      0.4331,
      1.0157,
      1.3063,
      0.3079,
      1.8787,
      0.2416,
      0.0096,
      0.0333,
      0.6917,
      0.3924,
      0.4676,
      0.4909,
      0.0495,
      0.5098,
      0.0002,
      0.1653,
      0.2434,
      0.3431,
      0.4342,
      2.5844,
      0.2903,
      1.968,
      2.0102,
      3.0217,
      0.6591,
      0.1771,
      0.1922,
      1.1933,
      1.2298,
      0.1982,
      0.0,
      0.0101,
      1.0269,
      1.8205,
      0.0018,
      0.5753,
      0.2113,
      0.014,
      0.2751,
      0.1594,
      0.0,
      0.0471,
      1.1242,
      1.084,
      0.0049,
      0.8476,
      1.3383,
      0.0534
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.5776,
    "entropy_zscore": -0.5718,
    "surprisal_zscore": -0.5718,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}