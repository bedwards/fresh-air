{
  "filename": "traditional_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:57.012122",
  "token_count": 70,
  "perplexity": {
    "overall": 1.5992,
    "per_token": [
      2.9571,
      3.4224,
      1.0,
      1.0026,
      1.1734,
      2.5031,
      2.7054,
      1.008,
      2.9856,
      2.3099,
      1.7692,
      3.0182,
      1.8196,
      1.0034,
      2.2522,
      2.4706,
      2.3464,
      1.6,
      2.2046,
      1.0002,
      1.7277,
      2.7936,
      1.2622,
      1.4155,
      1.0893,
      1.3644,
      1.0002,
      1.1148,
      11.2921,
      1.0603,
      2.135,
      1.5406,
      1.3838,
      1.4021,
      1.0806,
      1.5043,
      1.0012,
      1.089,
      1.0924,
      2.5525,
      1.9375,
      1.4359,
      1.0,
      1.0413,
      2.3276,
      1.7619,
      1.0844,
      1.9475,
      1.4017,
      1.2634,
      2.2326,
      2.8084,
      1.3384,
      1.4837,
      1.0981,
      1.5058,
      1.001,
      1.1274,
      1.188,
      4.0095,
      1.5632,
      2.0276,
      1.1851,
      1.459,
      1.1073,
      1.496,
      1.0004,
      1.2288,
      1.2788,
      1.9645
    ],
    "max": 11.2921,
    "min": 1.0,
    "mean": 1.8108
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.6773,
    "per_token": [
      1.5642,
      1.775,
      0.0,
      0.0037,
      0.2307,
      1.3237,
      1.4358,
      0.0114,
      1.578,
      1.2079,
      0.8231,
      1.5937,
      0.8637,
      0.0049,
      1.1713,
      1.3048,
      1.2305,
      0.678,
      1.1405,
      0.0002,
      0.7888,
      1.4821,
      0.336,
      0.5013,
      0.1234,
      0.4483,
      0.0003,
      0.1568,
      3.4972,
      0.0845,
      1.0943,
      0.6235,
      0.4686,
      0.4876,
      0.1118,
      0.5891,
      0.0017,
      0.123,
      0.1276,
      1.3519,
      0.9542,
      0.5219,
      0.0,
      0.0583,
      1.2188,
      0.8171,
      0.1168,
      0.9616,
      0.4872,
      0.3373,
      1.1587,
      1.4898,
      0.4205,
      0.5692,
      0.135,
      0.5906,
      0.0014,
      0.173,
      0.2486,
      2.0034,
      0.6445,
      1.0198,
      0.245,
      0.545,
      0.1471,
      0.5811,
      0.0006,
      0.2973,
      0.3548,
      0.9742
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.6006,
    "entropy_zscore": -0.5978,
    "surprisal_zscore": -0.5978,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}