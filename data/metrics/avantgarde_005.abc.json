{
  "filename": "avantgarde_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:44:56.380912",
  "token_count": 110,
  "perplexity": {
    "overall": 1.6341,
    "per_token": [
      2.0832,
      1.5789,
      1.4046,
      2.3161,
      2.2771,
      2.0369,
      1.0001,
      1.2289,
      2.456,
      1.6451,
      1.7034,
      4.0372,
      1.168,
      1.4003,
      1.0643,
      2.128,
      1.0198,
      1.4847,
      1.2445,
      2.9029,
      1.9393,
      6.104,
      2.0225,
      1.2882,
      1.0002,
      1.2147,
      1.2713,
      4.7268,
      1.0,
      1.1878,
      1.9037,
      3.2794,
      1.0639,
      1.8954,
      1.1025,
      1.2355,
      1.0,
      1.0627,
      1.505,
      2.9724,
      1.7633,
      2.8795,
      1.1021,
      1.9059,
      1.0797,
      1.2594,
      1.0001,
      1.0946,
      1.5532,
      2.7358,
      1.6773,
      3.2698,
      1.309,
      1.4558,
      1.1298,
      1.3185,
      1.0001,
      1.0673,
      4.4558,
      2.4167,
      1.8792,
      4.3704,
      1.1728,
      1.4071,
      1.0689,
      1.2732,
      1.0001,
      1.0824,
      1.4665,
      4.5471,
      1.9515,
      3.0913,
      1.0452,
      1.8303,
      1.0992,
      1.2781,
      1.0,
      1.0914,
      1.3185,
      2.0815,
      1.6994,
      3.3965,
      1.0733,
      1.4508,
      1.0693,
      1.6688,
      1.0003,
      1.1294,
      1.7021,
      2.5321,
      2.0704,
      4.0382,
      1.0802,
      1.5648,
      1.1231,
      1.3036,
      1.0001,
      1.0979,
      11.2291,
      1.0236,
      1.7394,
      3.9159,
      1.3272,
      1.45,
      1.0999,
      1.347,
      1.0001,
      1.0792,
      11.1666,
      1.0385
    ],
    "max": 11.2291,
    "min": 1.0,
    "mean": 1.9355
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7085,
    "per_token": [
      1.0588,
      0.6589,
      0.4902,
      1.2117,
      1.1872,
      1.0263,
      0.0001,
      0.2973,
      1.2963,
      0.7182,
      0.7684,
      2.0134,
      0.2241,
      0.4857,
      0.0899,
      1.0895,
      0.0282,
      0.5702,
      0.3156,
      1.5375,
      0.9556,
      2.6098,
      1.0161,
      0.3654,
      0.0002,
      0.2806,
      0.3463,
      2.2409,
      0.0,
      0.2483,
      0.9288,
      1.7134,
      0.0894,
      0.9225,
      0.1408,
      0.3051,
      0.0,
      0.0878,
      0.5898,
      1.5716,
      0.8183,
      1.5258,
      0.1402,
      0.9305,
      0.1107,
      0.3327,
      0.0001,
      0.1304,
      0.6352,
      1.452,
      0.7461,
      1.7092,
      0.3884,
      0.5418,
      0.1761,
      0.3989,
      0.0001,
      0.094,
      2.1557,
      1.273,
      0.9101,
      2.1278,
      0.2299,
      0.4927,
      0.0961,
      0.3485,
      0.0001,
      0.1142,
      0.5524,
      2.1849,
      0.9646,
      1.6282,
      0.0637,
      0.8721,
      0.1364,
      0.354,
      0.0,
      0.1262,
      0.3989,
      1.0576,
      0.765,
      1.7641,
      0.1021,
      0.5369,
      0.0966,
      0.7389,
      0.0004,
      0.1755,
      0.7673,
      1.3403,
      1.0499,
      2.0137,
      0.1112,
      0.646,
      0.1675,
      0.3825,
      0.0002,
      0.1347,
      3.4892,
      0.0337,
      0.7986,
      1.9694,
      0.4084,
      0.536,
      0.1374,
      0.4297,
      0.0001,
      0.1099,
      3.4811,
      0.0545
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.3411,
    "entropy_zscore": -0.3182,
    "surprisal_zscore": -0.3182,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}