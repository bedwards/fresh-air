{
  "filename": "silence_004.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:12.368396",
  "token_count": 50,
  "perplexity": {
    "overall": 1.6864,
    "per_token": [
      2.3844,
      2.4757,
      1.0466,
      4.6353,
      2.7764,
      3.3941,
      1.0003,
      1.3967,
      2.3314,
      3.6928,
      2.4145,
      1.7578,
      1.0747,
      1.0224,
      1.4296,
      3.4662,
      1.2782,
      1.4351,
      3.5207,
      2.4898,
      1.7659,
      1.0107,
      1.4922,
      1.0362,
      1.0689,
      1.0059,
      1.0003,
      1.0,
      1.5683,
      1.5189,
      2.6681,
      1.0854,
      1.0,
      1.0104,
      1.4739,
      3.513,
      1.7029,
      1.8059,
      1.2152,
      1.5321,
      1.9873,
      3.4666,
      1.2442,
      1.4516,
      1.0975,
      1.6073,
      1.001,
      1.4212,
      1.2242,
      7.8867
    ],
    "max": 7.8867,
    "min": 1.0,
    "mean": 1.9377
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.754,
    "per_token": [
      1.2536,
      1.3079,
      0.0657,
      2.2126,
      1.4732,
      1.763,
      0.0004,
      0.482,
      1.2212,
      1.8847,
      1.2717,
      0.8138,
      0.1039,
      0.032,
      0.5156,
      1.7933,
      0.3541,
      0.5212,
      1.8159,
      1.316,
      0.8204,
      0.0153,
      0.5774,
      0.0513,
      0.0962,
      0.0085,
      0.0005,
      0.0,
      0.6492,
      0.6031,
      1.4158,
      0.1182,
      0.0,
      0.0149,
      0.5596,
      1.8127,
      0.768,
      0.8527,
      0.2811,
      0.6155,
      0.9908,
      1.7935,
      0.3152,
      0.5376,
      0.1343,
      0.6847,
      0.0015,
      0.5072,
      0.2918,
      2.9794
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.0479,
    "entropy_zscore": 0.0896,
    "surprisal_zscore": 0.0896,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}