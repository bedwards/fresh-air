{
  "filename": "silence_001.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:02.890280",
  "token_count": 50,
  "perplexity": {
    "overall": 1.6837,
    "per_token": [
      2.3744,
      2.692,
      1.0357,
      7.154,
      5.2995,
      2.8259,
      2.399,
      2.4122,
      1.0008,
      1.0015,
      2.6156,
      1.6711,
      1.0776,
      1.0285,
      1.4265,
      3.1203,
      1.2704,
      1.4681,
      3.4699,
      2.3567,
      2.2243,
      1.5134,
      1.0092,
      1.4536,
      1.0322,
      1.126,
      1.0493,
      1.0001,
      1.0,
      2.0876,
      2.5081,
      1.4829,
      1.0,
      1.0412,
      1.876,
      3.3149,
      1.0084,
      1.4685,
      3.8008,
      1.0439,
      2.0085,
      1.3574,
      1.0,
      1.0669,
      2.1728,
      2.6967,
      1.0045,
      1.4301,
      2.8717,
      1.2149
    ],
    "max": 7.154,
    "min": 1.0,
    "mean": 1.9313
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7516,
    "per_token": [
      1.2475,
      1.4287,
      0.0506,
      2.8387,
      2.4058,
      1.4987,
      1.2624,
      1.2703,
      0.0012,
      0.0022,
      1.3872,
      0.7408,
      0.1078,
      0.0405,
      0.5124,
      1.6417,
      0.3452,
      0.5539,
      1.7949,
      1.2368,
      1.1534,
      0.5978,
      0.0132,
      0.5396,
      0.0458,
      0.1712,
      0.0694,
      0.0001,
      0.0,
      1.0619,
      1.3266,
      0.5684,
      0.0,
      0.0582,
      0.9077,
      1.729,
      0.0121,
      0.5544,
      1.9263,
      0.0619,
      1.0061,
      0.4409,
      0.0,
      0.0934,
      1.1196,
      1.4312,
      0.0065,
      0.5161,
      1.5219,
      0.2808
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.0278,
    "entropy_zscore": 0.0681,
    "surprisal_zscore": 0.0681,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}