{
  "filename": "traditional_001.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:37.722925",
  "token_count": 70,
  "perplexity": {
    "overall": 1.5358,
    "per_token": [
      1.8733,
      5.5076,
      1.0,
      1.0013,
      1.2375,
      1.8405,
      2.6078,
      1.0084,
      3.0414,
      1.8644,
      1.9342,
      2.2163,
      2.0351,
      1.003,
      2.1007,
      5.3178,
      3.471,
      1.6002,
      2.406,
      1.0001,
      2.0604,
      3.2469,
      1.2355,
      1.4358,
      1.0828,
      1.3302,
      1.0003,
      1.1284,
      1.1766,
      2.1163,
      1.5044,
      1.4395,
      1.0962,
      1.3948,
      1.0526,
      1.3034,
      1.0001,
      1.0383,
      1.0983,
      2.13,
      2.0233,
      1.0715,
      1.0,
      1.0028,
      1.9639,
      1.9836,
      1.1013,
      4.5631,
      1.1242,
      1.5803,
      2.0825,
      2.6371,
      1.2401,
      1.4041,
      1.0788,
      1.2948,
      1.0001,
      1.0612,
      1.0774,
      1.4837,
      2.6018,
      2.4724,
      1.1368,
      1.3502,
      1.1312,
      1.3555,
      1.0001,
      1.0359,
      1.0792,
      1.5663
    ],
    "max": 5.5076,
    "min": 1.0,
    "mean": 1.7063
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.619,
    "per_token": [
      0.9056,
      2.4614,
      0.0,
      0.0019,
      0.3074,
      0.8801,
      1.3828,
      0.0121,
      1.6047,
      0.8987,
      0.9517,
      1.1481,
      1.0251,
      0.0044,
      1.0708,
      2.4108,
      1.7953,
      0.6783,
      1.2666,
      0.0002,
      1.0429,
      1.699,
      0.3051,
      0.5218,
      0.1148,
      0.4117,
      0.0004,
      0.1742,
      0.2346,
      1.0816,
      0.5892,
      0.5256,
      0.1325,
      0.4801,
      0.074,
      0.3823,
      0.0002,
      0.0542,
      0.1352,
      1.0908,
      1.0167,
      0.0996,
      0.0,
      0.004,
      0.9737,
      0.9881,
      0.1392,
      2.19,
      0.1689,
      0.6602,
      1.0583,
      1.3989,
      0.3104,
      0.4897,
      0.1095,
      0.3727,
      0.0002,
      0.0857,
      0.1075,
      0.5692,
      1.3795,
      1.3059,
      0.185,
      0.4331,
      0.1778,
      0.4388,
      0.0001,
      0.0509,
      0.1099,
      0.6474
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -1.0721,
    "entropy_zscore": -1.1204,
    "surprisal_zscore": -1.1204,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}