{
  "filename": "experimental_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:45:31.282422",
  "token_count": 350,
  "perplexity": {
    "overall": 1.8744,
    "per_token": [
      2.4001,
      4.6467,
      1.8138,
      2.1682,
      1.0052,
      1.0001,
      2.9581,
      3.2265,
      3.9136,
      3.0712,
      1.6259,
      2.131,
      1.0,
      1.0705,
      2.2339,
      3.6695,
      1.0166,
      2.2345,
      2.0445,
      1.0771,
      1.2693,
      1.1179,
      1.0,
      1.0029,
      1.2414,
      2.5645,
      1.0157,
      1.6267,
      2.1396,
      1.0444,
      1.2585,
      1.1122,
      1.0,
      1.0051,
      1.3541,
      1.4743,
      1.0092,
      1.7029,
      2.2589,
      1.0421,
      1.7829,
      5.8498,
      1.2837,
      2.5421,
      3.8386,
      5.5371,
      1.5018,
      1.1826,
      1.1447,
      2.9028,
      2.7973,
      7.5482,
      1.4097,
      9.681,
      1.4917,
      6.8064,
      3.3115,
      2.1455,
      1.0487,
      1.3766,
      2.4989,
      5.4762,
      1.2776,
      1.3828,
      1.1037,
      1.2781,
      1.0001,
      1.0452,
      8.8829,
      1.0442,
      2.3528,
      5.2836,
      1.4522,
      10.9786,
      1.0069,
      9.608,
      6.0499,
      1.9332,
      9.0799,
      2.0849,
      2.2774,
      4.0128,
      3.5673,
      1.3281,
      1.0001,
      1.0864,
      1.3507,
      2.517,
      1.0514,
      4.4693,
      2.6174,
      5.8319,
      1.1228,
      1.8886,
      1.0894,
      1.1964,
      1.0001,
      1.0381,
      1.4127,
      3.8213,
      1.4263,
      3.891,
      1.7952,
      1.2771,
      1.0001,
      1.0691,
      9.4586,
      1.0515,
      5.626,
      1.0136,
      2.4851,
      4.7151,
      1.5113,
      5.2196,
      1.7957,
      7.8318,
      2.7601,
      1.6249,
      1.0817,
      1.3247,
      3.0763,
      7.5117,
      1.1936,
      2.3859,
      2.3915,
      5.2621,
      2.8284,
      1.0094,
      4.9073,
      9.1779,
      1.6897,
      4.5845,
      3.1003,
      1.2126,
      1.0002,
      1.1663,
      1.3659,
      2.62,
      1.0003,
      1.5748,
      1.8043,
      4.3735,
      1.1106,
      1.7519,
      1.0821,
      1.3968,
      1.0003,
      1.1088,
      1.4182,
      2.577,
      1.4299,
      5.8651,
      2.1706,
      1.3387,
      1.0003,
      1.1694,
      1.3606,
      4.689,
      1.1232,
      6.6543,
      1.6925,
      3.6033,
      1.8982,
      1.3037,
      1.0006,
      1.1668,
      1.2375,
      3.8518,
      1.0006,
      1.681,
      1.4881,
      3.9046,
      2.1126,
      1.2951,
      1.0001,
      1.1141,
      1.2585,
      5.5901,
      1.0003,
      1.2909,
      1.3142,
      4.0249,
      1.0896,
      1.3315,
      1.0568,
      1.432,
      1.0001,
      1.1978,
      1.3852,
      3.6449,
      1.9574,
      1.4775,
      1.0,
      1.024,
      2.0473,
      1.9079,
      1.1538,
      1.7225,
      1.2654,
      2.8909,
      1.9099,
      4.013,
      1.1287,
      1.5142,
      1.0381,
      1.6639,
      1.0003,
      1.096,
      1.9376,
      1.7839,
      3.2881,
      8.7647,
      1.3797,
      7.7355,
      1.5943,
      6.459,
      2.1665,
      1.0562,
      1.0857,
      2.4181,
      1.8831,
      4.6426,
      1.8567,
      1.2981,
      1.0002,
      1.2295,
      10.0923,
      1.1113,
      1.182,
      3.5671,
      1.6891,
      5.9651,
      1.314,
      1.458,
      1.0993,
      1.3376,
      1.0001,
      1.0895,
      1.8966,
      4.1939,
      2.5127,
      3.6357,
      2.2655,
      1.278,
      1.0002,
      1.1524,
      1.3098,
      3.4753,
      1.0642,
      4.1181,
      1.5356,
      4.1762,
      1.1468,
      1.4771,
      1.0749,
      1.7165,
      1.001,
      1.2878,
      1.9039,
      1.8514,
      2.1091,
      3.5759,
      3.4601,
      1.2813,
      1.0002,
      1.1057,
      1.2501,
      3.543,
      1.0828,
      6.0192,
      2.6084,
      4.1266,
      2.7499,
      1.2739,
      1.0001,
      1.1221,
      1.3158,
      3.7596,
      1.0004,
      1.6466,
      1.6155,
      4.9219,
      1.0991,
      1.6121,
      1.056,
      1.301,
      1.0001,
      1.1149,
      1.3235,
      3.9337,
      2.1579,
      6.3355,
      2.4513,
      1.7132,
      1.0015,
      1.3358,
      1.1513,
      4.9354,
      1.0015,
      1.6789,
      2.3019,
      3.9989,
      3.4723,
      1.3966,
      1.0005,
      1.2571,
      1.1818,
      2.6609,
      1.0004,
      1.6481,
      1.7313,
      4.4905,
      1.1434,
      1.7267,
      1.0434,
      1.3897,
      1.0003,
      1.1971,
      1.501,
      2.6828,
      1.9472,
      5.3043,
      1.1536,
      1.4857,
      1.0551,
      1.388,
      1.0002,
      1.124,
      8.568,
      2.9504,
      1.8376,
      3.5283,
      2.2899,
      1.2677,
      1.0001,
      1.2936,
      1.2612,
      3.5917,
      1.0004,
      1.2976,
      1.7661,
      4.1017,
      1.1318,
      1.6828,
      1.0982,
      1.4157,
      1.0004,
      1.1741,
      1.7003,
      2.5827
    ],
    "max": 10.9786,
    "min": 1.0,
    "mean": 2.336
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.9064,
    "per_token": [
      1.2631,
      2.2162,
      0.859,
      1.1165,
      0.0075,
      0.0002,
      1.5647,
      1.69,
      1.9685,
      1.6188,
      0.7013,
      1.0915,
      0.0,
      0.0983,
      1.1596,
      1.8756,
      0.0238,
      1.1599,
      1.0318,
      0.1071,
      0.3441,
      0.1608,
      0.0,
      0.0042,
      0.312,
      1.3587,
      0.0225,
      0.702,
      1.0973,
      0.0627,
      0.3317,
      0.1534,
      0.0,
      0.0074,
      0.4373,
      0.56,
      0.0132,
      0.768,
      1.1756,
      0.0595,
      0.8343,
      2.5484,
      0.3603,
      1.346,
      1.9406,
      2.4691,
      0.5867,
      0.242,
      0.1949,
      1.5375,
      1.484,
      2.9161,
      0.4954,
      3.2752,
      0.577,
      2.7669,
      1.7275,
      1.1013,
      0.0686,
      0.4611,
      1.3213,
      2.4532,
      0.3534,
      0.4676,
      0.1423,
      0.3541,
      0.0001,
      0.0638,
      3.151,
      0.0624,
      1.2344,
      2.4015,
      0.5382,
      3.4566,
      0.0099,
      3.2642,
      2.5969,
      0.951,
      3.1827,
      1.06,
      1.1874,
      2.0046,
      1.8348,
      0.4094,
      0.0002,
      0.1195,
      0.4337,
      1.3317,
      0.0723,
      2.16,
      1.3881,
      2.544,
      0.1671,
      0.9173,
      0.1235,
      0.2587,
      0.0001,
      0.054,
      0.4985,
      1.934,
      0.5123,
      1.9601,
      0.8441,
      0.3528,
      0.0002,
      0.0965,
      3.2416,
      0.0725,
      2.4921,
      0.0195,
      1.3133,
      2.2373,
      0.5958,
      2.384,
      0.8446,
      2.9693,
      1.4647,
      0.7004,
      0.1133,
      0.4056,
      1.6212,
      2.9091,
      0.2553,
      1.2545,
      1.2579,
      2.3956,
      1.5,
      0.0135,
      2.2949,
      3.1982,
      0.7568,
      2.1968,
      1.6324,
      0.2781,
      0.0003,
      0.2219,
      0.4499,
      1.3896,
      0.0004,
      0.6552,
      0.8514,
      2.1288,
      0.1514,
      0.8089,
      0.1138,
      0.4821,
      0.0004,
      0.149,
      0.5041,
      1.3657,
      0.5159,
      2.5521,
      1.1181,
      0.4209,
      0.0004,
      0.2258,
      0.4443,
      2.2293,
      0.1676,
      2.7343,
      0.7592,
      1.8493,
      0.9246,
      0.3826,
      0.0008,
      0.2226,
      0.3074,
      1.9455,
      0.0009,
      0.7493,
      0.5735,
      1.9652,
      1.079,
      0.3731,
      0.0001,
      0.1559,
      0.3317,
      2.4829,
      0.0005,
      0.3684,
      0.3942,
      2.009,
      0.1238,
      0.413,
      0.0797,
      0.518,
      0.0002,
      0.2604,
      0.4701,
      1.8659,
      0.9689,
      0.5631,
      0.0,
      0.0342,
      1.0337,
      0.932,
      0.2064,
      0.7845,
      0.3395,
      1.5315,
      0.9335,
      2.0047,
      0.1746,
      0.5985,
      0.0539,
      0.7345,
      0.0004,
      0.1323,
      0.9543,
      0.835,
      1.7172,
      3.1317,
      0.4644,
      2.9515,
      0.6729,
      2.6913,
      1.1153,
      0.0789,
      0.1187,
      1.2739,
      0.9131,
      2.2149,
      0.8927,
      0.3764,
      0.0003,
      0.298,
      3.3352,
      0.1522,
      0.2412,
      1.8347,
      0.7562,
      2.5765,
      0.394,
      0.544,
      0.1366,
      0.4196,
      0.0002,
      0.1237,
      0.9235,
      2.0683,
      1.3292,
      1.8622,
      1.1798,
      0.3539,
      0.0002,
      0.2046,
      0.3893,
      1.7971,
      0.0898,
      2.042,
      0.6188,
      2.0622,
      0.1976,
      0.5628,
      0.1042,
      0.7795,
      0.0014,
      0.3649,
      0.929,
      0.8886,
      1.0766,
      1.8383,
      1.7908,
      0.3576,
      0.0003,
      0.1449,
      0.322,
      1.825,
      0.1147,
      2.5896,
      1.3832,
      2.045,
      1.4594,
      0.3493,
      0.0002,
      0.1662,
      0.396,
      1.9106,
      0.0006,
      0.7195,
      0.692,
      2.2992,
      0.1363,
      0.689,
      0.0787,
      0.3797,
      0.0001,
      0.1569,
      0.4044,
      1.9759,
      1.1096,
      2.6635,
      1.2936,
      0.7767,
      0.0022,
      0.4177,
      0.2032,
      2.3032,
      0.0022,
      0.7475,
      1.2028,
      1.9996,
      1.7959,
      0.4819,
      0.0008,
      0.3301,
      0.2409,
      1.4119,
      0.0006,
      0.7208,
      0.7919,
      2.1669,
      0.1933,
      0.788,
      0.0613,
      0.4748,
      0.0004,
      0.2596,
      0.586,
      1.4237,
      0.9614,
      2.4072,
      0.2061,
      0.5712,
      0.0774,
      0.473,
      0.0002,
      0.1687,
      3.099,
      1.5609,
      0.8779,
      1.819,
      1.1953,
      0.3422,
      0.0002,
      0.3714,
      0.3348,
      1.8447,
      0.0006,
      0.3758,
      0.8206,
      2.0362,
      0.1786,
      0.7509,
      0.1352,
      0.5015,
      0.0006,
      0.2316,
      0.7658,
      1.3689
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.4461,
    "entropy_zscore": 1.4556,
    "surprisal_zscore": 1.4556,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}