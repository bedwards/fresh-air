{
  "filename": "avantgarde_001.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:44:39.143766",
  "token_count": 60,
  "perplexity": {
    "overall": 1.796,
    "per_token": [
      2.2833,
      1.6367,
      1.472,
      3.2226,
      2.1926,
      3.0693,
      1.0001,
      1.4377,
      2.4462,
      2.7696,
      2.0972,
      5.3699,
      1.2402,
      1.4784,
      1.0637,
      2.264,
      1.0003,
      1.1665,
      1.4003,
      4.0889,
      2.7618,
      4.2748,
      1.0852,
      1.4211,
      1.0539,
      1.5398,
      1.0001,
      1.0711,
      1.5373,
      3.93,
      3.2615,
      1.5105,
      1.0,
      1.0426,
      1.9461,
      1.8285,
      1.091,
      2.1196,
      1.203,
      1.1713,
      1.8254,
      1.1898,
      1.0,
      1.0149,
      1.9957,
      1.87,
      10.9758,
      1.9954,
      1.0535,
      3.1491,
      1.9934,
      3.822,
      1.2172,
      1.4641,
      1.1081,
      1.3629,
      1.0001,
      1.0984,
      9.3548,
      3.2778
    ],
    "max": 10.9758,
    "min": 1.0,
    "mean": 2.172
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8448,
    "per_token": [
      1.1911,
      0.7108,
      0.5578,
      1.6882,
      1.1326,
      1.6179,
      0.0001,
      0.5238,
      1.2905,
      1.4697,
      1.0685,
      2.4249,
      0.3106,
      0.564,
      0.0892,
      1.1789,
      0.0005,
      0.2221,
      0.4857,
      2.0317,
      1.4656,
      2.0958,
      0.118,
      0.507,
      0.0757,
      0.6228,
      0.0002,
      0.099,
      0.6204,
      1.9745,
      1.7055,
      0.5951,
      0.0,
      0.0602,
      0.9606,
      0.8707,
      0.1257,
      1.0838,
      0.2666,
      0.2281,
      0.8683,
      0.2507,
      0.0,
      0.0213,
      0.9969,
      0.9031,
      3.4563,
      0.9967,
      0.0753,
      1.655,
      0.9952,
      1.9343,
      0.2836,
      0.55,
      0.1481,
      0.4467,
      0.0002,
      0.1354,
      3.2257,
      1.7127
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.863,
    "entropy_zscore": 0.9035,
    "surprisal_zscore": 0.9035,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}