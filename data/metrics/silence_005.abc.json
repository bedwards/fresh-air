{
  "filename": "silence_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:15.747554",
  "token_count": 60,
  "perplexity": {
    "overall": 1.7859,
    "per_token": [
      2.3482,
      2.3016,
      1.0464,
      3.917,
      3.1241,
      3.0665,
      1.0002,
      1.4363,
      2.3524,
      4.425,
      2.6503,
      2.0231,
      1.076,
      1.0111,
      1.3759,
      4.7026,
      1.1497,
      1.5125,
      2.8608,
      2.7479,
      2.8415,
      1.0111,
      1.8456,
      1.0285,
      1.0495,
      1.003,
      1.0003,
      1.0001,
      1.7517,
      1.4362,
      1.6505,
      4.1186,
      1.3011,
      1.5608,
      1.1728,
      2.3183,
      1.0041,
      1.842,
      2.4377,
      1.0,
      2.2549,
      1.5529,
      1.0,
      1.2486,
      2.558,
      3.902,
      1.0,
      1.7762,
      4.75,
      1.5686,
      2.472,
      3.6881,
      1.3421,
      1.6399,
      1.3106,
      2.4184,
      1.0547,
      3.2965,
      2.5233,
      1.0001
    ],
    "max": 4.75,
    "min": 1.0,
    "mean": 2.0143
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8366,
    "per_token": [
      1.2316,
      1.2026,
      0.0655,
      1.9698,
      1.6434,
      1.6166,
      0.0003,
      0.5224,
      1.2341,
      2.1457,
      1.4062,
      1.0165,
      0.1057,
      0.016,
      0.4604,
      2.2334,
      0.2013,
      0.597,
      1.5164,
      1.4583,
      1.5067,
      0.0159,
      0.8841,
      0.0405,
      0.0697,
      0.0043,
      0.0004,
      0.0001,
      0.8088,
      0.5223,
      0.7229,
      2.0421,
      0.3798,
      0.6423,
      0.23,
      1.2131,
      0.0059,
      0.8813,
      1.2855,
      0.0001,
      1.1731,
      0.635,
      0.0,
      0.3203,
      1.355,
      1.9642,
      0.0,
      0.8288,
      2.2479,
      0.6494,
      1.3057,
      1.8829,
      0.4245,
      0.7136,
      0.3902,
      1.2741,
      0.0769,
      1.7209,
      1.3353,
      0.0001
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.7879,
    "entropy_zscore": 0.83,
    "surprisal_zscore": 0.83,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}