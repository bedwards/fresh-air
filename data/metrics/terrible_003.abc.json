{
  "filename": "terrible_003.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:26.721310",
  "token_count": 70,
  "perplexity": {
    "overall": 1.8225,
    "per_token": [
      1.429,
      1.7576,
      1.0,
      1.0368,
      1.6519,
      1.4103,
      1.7066,
      2.1376,
      6.2846,
      2.535,
      2.1044,
      1.4667,
      1.0,
      1.0088,
      1.4672,
      2.6711,
      1.0269,
      2.1657,
      4.3913,
      1.0019,
      3.043,
      4.0596,
      1.4384,
      1.4608,
      1.0635,
      1.626,
      1.0253,
      1.6501,
      1.6871,
      3.0129,
      2.7817,
      4.335,
      1.1094,
      1.3443,
      1.0523,
      2.1089,
      1.0285,
      1.5683,
      1.4242,
      6.9154,
      2.7287,
      3.9386,
      1.3036,
      2.0382,
      2.6734,
      1.0743,
      6.3786,
      1.5655,
      1.6558,
      1.38,
      2.4897,
      4.8622,
      1.0,
      1.2511,
      1.5039,
      2.2158,
      2.1599,
      1.3037,
      2.0073,
      1.4587,
      1.9001,
      5.281,
      1.2006,
      1.9483,
      1.1183,
      1.5017,
      1.0004,
      1.1701,
      1.453,
      3.2794
    ],
    "max": 6.9154,
    "min": 1.0,
    "mean": 2.1119
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8659,
    "per_token": [
      0.515,
      0.8136,
      0.0,
      0.0521,
      0.7241,
      0.496,
      0.7712,
      1.096,
      2.6518,
      1.342,
      1.0734,
      0.5526,
      0.0,
      0.0126,
      0.553,
      1.4174,
      0.0382,
      1.1148,
      2.1346,
      0.0028,
      1.6055,
      2.0213,
      0.5244,
      0.5467,
      0.0888,
      0.7014,
      0.036,
      0.7226,
      0.7546,
      1.5911,
      1.476,
      2.116,
      0.1498,
      0.4268,
      0.0736,
      1.0765,
      0.0405,
      0.6492,
      0.5102,
      2.7898,
      1.4482,
      1.9777,
      0.3825,
      1.0273,
      1.4187,
      0.1034,
      2.6732,
      0.6466,
      0.7275,
      0.4647,
      1.316,
      2.2816,
      0.0,
      0.3232,
      0.5887,
      1.1478,
      1.1109,
      0.3826,
      1.0053,
      0.5447,
      0.9261,
      2.4008,
      0.2637,
      0.9622,
      0.1613,
      0.5866,
      0.0005,
      0.2266,
      0.539,
      1.7134
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.0601,
    "entropy_zscore": 1.0926,
    "surprisal_zscore": 1.0926,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}