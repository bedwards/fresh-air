{
  "filename": "terrible_001.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:19.005987",
  "token_count": 60,
  "perplexity": {
    "overall": 1.6251,
    "per_token": [
      1.1698,
      1.5218,
      1.0,
      1.031,
      1.7318,
      1.7392,
      1.4597,
      4.7923,
      3.3028,
      3.8307,
      2.3877,
      1.5788,
      1.1617,
      2.4601,
      1.0334,
      1.2159,
      2.7538,
      1.1342,
      3.5592,
      1.4585,
      2.1939,
      3.5931,
      1.7035,
      4.4563,
      1.0035,
      1.4464,
      1.0624,
      1.0001,
      2.8789,
      1.0334,
      2.4903,
      3.318,
      1.0656,
      1.5799,
      1.0499,
      1.8564,
      1.0003,
      1.0963,
      1.694,
      2.2733,
      1.3444,
      2.443,
      1.1827,
      1.5376,
      1.0887,
      1.3966,
      1.0002,
      1.1376,
      1.4412,
      3.0323,
      1.231,
      1.3844,
      1.0,
      1.0267,
      1.5496,
      1.3767,
      1.0082,
      1.8416,
      4.0226,
      1.0577
    ],
    "max": 4.7923,
    "min": 1.0,
    "mean": 1.8203
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7005,
    "per_token": [
      0.2263,
      0.6058,
      0.0,
      0.0441,
      0.7922,
      0.7984,
      0.5457,
      2.2607,
      1.7237,
      1.9376,
      1.2556,
      0.6588,
      0.2163,
      1.2987,
      0.0475,
      0.2821,
      1.4614,
      0.1817,
      1.8316,
      0.5444,
      1.1335,
      1.8452,
      0.7685,
      2.1559,
      0.0051,
      0.5324,
      0.0874,
      0.0002,
      1.5255,
      0.0473,
      1.3163,
      1.7303,
      0.0916,
      0.6598,
      0.0702,
      0.8925,
      0.0004,
      0.1326,
      0.7604,
      1.1848,
      0.427,
      1.2887,
      0.2421,
      0.6207,
      0.1226,
      0.4819,
      0.0003,
      0.186,
      0.5273,
      1.6004,
      0.2998,
      0.4693,
      0.0,
      0.0381,
      0.6319,
      0.4612,
      0.0118,
      0.881,
      2.0081,
      0.0809
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.408,
    "entropy_zscore": -0.3899,
    "surprisal_zscore": -0.3899,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}