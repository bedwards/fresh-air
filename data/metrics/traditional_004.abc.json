{
  "filename": "traditional_004.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:53.021939",
  "token_count": 80,
  "perplexity": {
    "overall": 1.9295,
    "per_token": [
      3.2865,
      4.6605,
      1.191,
      2.0346,
      1.001,
      2.6425,
      2.6386,
      1.32,
      1.0161,
      2.3811,
      2.0804,
      3.0481,
      1.583,
      1.7198,
      1.134,
      1.8573,
      1.0281,
      1.6984,
      1.7399,
      3.2521,
      2.6983,
      6.0067,
      1.9955,
      12.7913,
      3.3994,
      1.2899,
      7.9189,
      1.0624,
      1.1206,
      2.3416,
      1.3685,
      4.8866,
      1.0,
      1.0569,
      1.6431,
      2.3589,
      1.4288,
      1.0,
      2.4038,
      2.6122,
      2.8263,
      1.039,
      1.0,
      1.0018,
      2.1053,
      2.9826,
      1.0166,
      1.6447,
      1.5986,
      1.3722,
      1.7351,
      6.774,
      1.1399,
      1.6568,
      3.3766,
      1.0,
      1.4007,
      1.0,
      1.6231,
      2.5321,
      2.5977,
      4.5621,
      3.2404,
      2.9191,
      1.0003,
      1.2963,
      1.2898,
      3.2799,
      1.1014,
      6.0117,
      2.1804,
      3.2929,
      1.0554,
      2.7325,
      1.2912,
      1.3011,
      2.2711,
      1.4121,
      1.7413,
      1.0113
    ],
    "max": 12.7913,
    "min": 1.0,
    "mean": 2.3264
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.9482,
    "per_token": [
      1.7165,
      2.2205,
      0.2522,
      1.0248,
      0.0014,
      1.4019,
      1.3998,
      0.4005,
      0.0231,
      1.2516,
      1.0568,
      1.6079,
      0.6627,
      0.7823,
      0.1815,
      0.8932,
      0.04,
      0.7642,
      0.799,
      1.7014,
      1.432,
      2.5866,
      0.9967,
      3.6771,
      1.7653,
      0.3673,
      2.9853,
      0.0873,
      0.1643,
      1.2275,
      0.4526,
      2.2888,
      0.0001,
      0.0798,
      0.7164,
      1.2381,
      0.5149,
      0.0,
      1.2653,
      1.3853,
      1.4989,
      0.0552,
      0.0,
      0.0025,
      1.074,
      1.5766,
      0.0238,
      0.7178,
      0.6768,
      0.4565,
      0.795,
      2.76,
      0.1889,
      0.7284,
      1.7556,
      0.0,
      0.4862,
      0.0,
      0.6987,
      1.3403,
      1.3772,
      2.1897,
      1.6962,
      1.5455,
      0.0004,
      0.3744,
      0.3672,
      1.7136,
      0.1394,
      2.5878,
      1.1246,
      1.7193,
      0.0778,
      1.4502,
      0.3687,
      0.3797,
      1.1834,
      0.4978,
      0.8002,
      0.0162
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.8558,
    "entropy_zscore": 1.8302,
    "surprisal_zscore": 1.8302,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}