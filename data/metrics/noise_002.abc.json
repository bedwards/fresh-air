{
  "filename": "noise_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:46:36.224008",
  "token_count": 190,
  "perplexity": {
    "overall": 1.706,
    "per_token": [
      3.2168,
      1.6203,
      1.0,
      1.0484,
      1.9453,
      4.1418,
      1.0097,
      1.8755,
      3.5714,
      3.1716,
      3.2964,
      1.759,
      1.0,
      1.0325,
      2.2425,
      2.0444,
      1.4906,
      1.6379,
      1.8831,
      3.745,
      1.8397,
      1.9295,
      1.0,
      1.0043,
      1.2171,
      4.698,
      1.0279,
      2.5781,
      1.6387,
      1.0708,
      1.9773,
      1.8717,
      1.0,
      1.2415,
      2.1917,
      3.889,
      1.2303,
      2.4734,
      1.1155,
      2.0023,
      1.3171,
      2.2433,
      1.1691,
      1.4599,
      1.2513,
      2.2033,
      1.0013,
      1.3915,
      1.4711,
      4.1154,
      2.1224,
      4.0686,
      2.2185,
      1.3277,
      1.0002,
      1.1669,
      1.2258,
      3.1455,
      1.0603,
      2.5026,
      1.4808,
      1.3423,
      1.0022,
      1.0003,
      2.0006,
      4.7778,
      5.1724,
      3.7145,
      1.0891,
      2.9515,
      1.5578,
      4.3163,
      1.4049,
      1.4307,
      1.1097,
      1.458,
      1.0002,
      1.1669,
      1.2817,
      3.6801,
      1.6641,
      3.3877,
      1.1048,
      1.3657,
      1.0863,
      1.6255,
      1.0004,
      1.098,
      1.2502,
      2.7823,
      1.2987,
      3.32,
      1.1202,
      1.4444,
      1.1153,
      1.3911,
      1.0001,
      1.121,
      1.3938,
      2.5319,
      1.7095,
      3.471,
      1.0643,
      1.5972,
      1.1114,
      1.4726,
      1.0002,
      1.1753,
      1.3763,
      3.9821,
      2.1047,
      2.9622,
      1.2203,
      1.6088,
      1.138,
      1.3038,
      1.0002,
      1.0932,
      1.4297,
      2.2749,
      2.1856,
      4.9098,
      1.3768,
      1.3468,
      1.1373,
      1.4903,
      1.0008,
      1.2477,
      1.3541,
      5.0504,
      2.4554,
      5.5534,
      1.1139,
      1.3803,
      1.0777,
      1.6306,
      1.0004,
      1.1017,
      1.4726,
      2.3957,
      2.5417,
      4.2962,
      2.2611,
      1.3304,
      1.0004,
      1.1369,
      1.1945,
      4.5763,
      1.1956,
      6.1031,
      2.2099,
      3.9673,
      1.083,
      1.4501,
      1.053,
      1.916,
      1.0003,
      1.0692,
      1.3371,
      4.4156,
      2.0927,
      1.7924,
      1.0,
      1.2154,
      1.7962,
      3.0371,
      1.131,
      2.8921,
      1.2328,
      1.9419,
      2.0866,
      2.6146,
      2.7467,
      1.2759,
      1.0002,
      1.1557,
      1.2801,
      2.4142,
      1.111,
      4.7954,
      1.7884,
      4.55,
      1.1454,
      1.478,
      1.1913,
      1.3211,
      1.0001,
      1.0686,
      1.6338,
      4.2779
    ],
    "max": 6.1031,
    "min": 1.0,
    "mean": 1.9464
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7706,
    "per_token": [
      1.6856,
      0.6962,
      0.0,
      0.0682,
      0.96,
      2.0502,
      0.014,
      0.9073,
      1.8365,
      1.6652,
      1.7209,
      0.8147,
      0.0,
      0.0461,
      1.1651,
      1.0317,
      0.5759,
      0.7119,
      0.9131,
      1.905,
      0.8795,
      0.9482,
      0.0,
      0.0062,
      0.2835,
      2.232,
      0.0397,
      1.3663,
      0.7126,
      0.0986,
      0.9835,
      0.9044,
      0.0,
      0.3121,
      1.132,
      1.9594,
      0.2991,
      1.3065,
      0.1577,
      1.0017,
      0.3974,
      1.1656,
      0.2254,
      0.5458,
      0.3234,
      1.1396,
      0.0018,
      0.4766,
      0.5569,
      2.041,
      1.0857,
      2.0245,
      1.1496,
      0.409,
      0.0003,
      0.2227,
      0.2937,
      1.6533,
      0.0845,
      1.3234,
      0.5664,
      0.4247,
      0.0032,
      0.0004,
      1.0004,
      2.2563,
      2.3708,
      1.8932,
      0.1232,
      1.5614,
      0.6395,
      2.1098,
      0.4905,
      0.5167,
      0.1502,
      0.544,
      0.0002,
      0.2227,
      0.3581,
      1.8797,
      0.7347,
      1.7603,
      0.1438,
      0.4497,
      0.1195,
      0.7009,
      0.0005,
      0.1348,
      0.3222,
      1.4763,
      0.377,
      1.7312,
      0.1638,
      0.5304,
      0.1575,
      0.4762,
      0.0002,
      0.1648,
      0.479,
      1.3402,
      0.7736,
      1.7954,
      0.0899,
      0.6755,
      0.1524,
      0.5584,
      0.0003,
      0.233,
      0.4608,
      1.9935,
      1.0736,
      1.5667,
      0.2872,
      0.686,
      0.1865,
      0.3827,
      0.0003,
      0.1285,
      0.5158,
      1.1858,
      1.128,
      2.2957,
      0.4613,
      0.4295,
      0.1857,
      0.5756,
      0.0012,
      0.3193,
      0.4373,
      2.3364,
      1.2959,
      2.4734,
      0.1557,
      0.465,
      0.1079,
      0.7054,
      0.0005,
      0.1398,
      0.5583,
      1.2604,
      1.3458,
      2.1031,
      1.1771,
      0.4118,
      0.0005,
      0.1851,
      0.2564,
      2.1942,
      0.2577,
      2.6095,
      1.144,
      1.9881,
      0.115,
      0.5361,
      0.0745,
      0.9381,
      0.0004,
      0.0966,
      0.4191,
      2.1426,
      1.0654,
      0.8419,
      0.0,
      0.2814,
      0.8449,
      1.6027,
      0.1775,
      1.5321,
      0.302,
      0.9575,
      1.0612,
      1.3866,
      1.4577,
      0.3515,
      0.0002,
      0.2088,
      0.3563,
      1.2715,
      0.1519,
      2.2616,
      0.8387,
      2.1859,
      0.1958,
      0.5637,
      0.2525,
      0.4018,
      0.0002,
      0.0957,
      0.7083,
      2.0969
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.1937,
    "entropy_zscore": 0.2384,
    "surprisal_zscore": 0.2384,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}