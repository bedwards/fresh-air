{
  "filename": "traditional_003.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:48.559156",
  "token_count": 90,
  "perplexity": {
    "overall": 1.6838,
    "per_token": [
      2.7707,
      5.249,
      1.5778,
      3.8132,
      1.026,
      2.9157,
      3.0102,
      1.8356,
      2.96,
      2.0939,
      1.7039,
      4.1238,
      1.2891,
      1.4985,
      1.1179,
      2.1863,
      1.021,
      1.9201,
      1.3732,
      2.6033,
      2.1256,
      2.3381,
      1.6828,
      1.4371,
      1.1193,
      1.6838,
      1.0015,
      1.3002,
      1.1283,
      3.4354,
      1.5958,
      2.2645,
      1.6684,
      2.2684,
      1.0044,
      1.4287,
      1.1239,
      1.0004,
      4.7181,
      1.0861,
      2.5435,
      2.4668,
      1.773,
      2.8454,
      1.0552,
      2.0749,
      1.1545,
      1.0004,
      2.4882,
      2.3124,
      1.6119,
      1.0771,
      1.43,
      2.7334,
      1.0078,
      1.5122,
      1.1148,
      1.0002,
      3.6926,
      1.5521,
      1.9291,
      2.3852,
      1.911,
      1.5039,
      1.0969,
      1.5151,
      1.0016,
      1.1325,
      1.2585,
      2.8572,
      2.5099,
      1.4348,
      1.4143,
      1.4671,
      1.0733,
      1.7051,
      1.0015,
      1.1574,
      1.0915,
      2.4903,
      2.5826,
      2.4944,
      1.5551,
      1.4445,
      1.0707,
      1.5736,
      1.0018,
      1.2116,
      1.1572,
      2.1368
    ],
    "max": 5.249,
    "min": 1.0002,
    "mean": 1.8457
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7517,
    "per_token": [
      1.4703,
      2.3921,
      0.6579,
      1.931,
      0.037,
      1.5438,
      1.5899,
      0.8763,
      1.5656,
      1.0662,
      0.7688,
      2.044,
      0.3663,
      0.5835,
      0.1608,
      1.1285,
      0.03,
      0.9412,
      0.4576,
      1.3804,
      1.0879,
      1.2254,
      0.7508,
      0.5232,
      0.1627,
      0.7517,
      0.0022,
      0.3787,
      0.1741,
      1.7805,
      0.6743,
      1.1792,
      0.7384,
      1.1817,
      0.0064,
      0.5147,
      0.1685,
      0.0005,
      2.2382,
      0.1191,
      1.3468,
      1.3027,
      0.8262,
      1.5086,
      0.0775,
      1.0531,
      0.2072,
      0.0005,
      1.3151,
      1.2094,
      0.6887,
      0.1072,
      0.516,
      1.4507,
      0.0112,
      0.5966,
      0.1567,
      0.0002,
      1.8846,
      0.6342,
      0.9479,
      1.2541,
      0.9343,
      0.5887,
      0.1335,
      0.5994,
      0.0023,
      0.1795,
      0.3317,
      1.5146,
      1.3276,
      0.5209,
      0.5001,
      0.553,
      0.1021,
      0.7699,
      0.0021,
      0.2109,
      0.1263,
      1.3163,
      1.3688,
      1.3187,
      0.6371,
      0.5306,
      0.0986,
      0.6541,
      0.0025,
      0.2769,
      0.2106,
      1.0954
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.0286,
    "entropy_zscore": 0.069,
    "surprisal_zscore": 0.069,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}