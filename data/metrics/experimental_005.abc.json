{
  "filename": "experimental_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:46:17.130570",
  "token_count": 210,
  "perplexity": {
    "overall": 1.8386,
    "per_token": [
      2.5179,
      3.8906,
      1.1244,
      3.5404,
      3.5835,
      2.5247,
      1.0005,
      1.3188,
      2.2853,
      5.4709,
      1.5794,
      2.0964,
      1.0,
      1.0659,
      2.2165,
      3.858,
      1.0179,
      2.2796,
      1.9183,
      1.0698,
      1.2693,
      1.1179,
      1.0,
      1.0029,
      1.2414,
      2.5645,
      1.0157,
      1.6267,
      2.1396,
      1.0444,
      1.2585,
      1.1122,
      1.0,
      1.0051,
      1.3541,
      1.4743,
      1.0092,
      1.7029,
      2.2589,
      1.0421,
      1.6562,
      8.7682,
      1.0,
      1.1855,
      1.5578,
      2.1361,
      1.005,
      1.0005,
      4.037,
      1.2653,
      1.4904,
      3.8227,
      2.4562,
      1.3023,
      1.0001,
      1.238,
      1.2391,
      3.6646,
      1.4086,
      3.9629,
      2.2349,
      3.9666,
      2.857,
      1.3796,
      1.0001,
      1.1579,
      1.2727,
      4.055,
      1.0001,
      1.4404,
      2.5454,
      5.3369,
      4.0977,
      1.2514,
      1.0001,
      1.0511,
      1.2889,
      2.2398,
      1.0657,
      4.4194,
      1.7516,
      1.7153,
      1.0384,
      1.6461,
      1.0398,
      1.2613,
      1.0001,
      1.0864,
      1.3244,
      1.9404,
      2.0075,
      4.9733,
      1.0743,
      1.488,
      1.0482,
      1.6293,
      1.0001,
      1.0363,
      1.8208,
      1.8998,
      1.7165,
      5.3706,
      1.1471,
      1.4188,
      1.0673,
      1.6369,
      1.0003,
      1.0622,
      1.9923,
      3.5023,
      1.824,
      3.4584,
      2.3741,
      1.3096,
      1.0002,
      1.2522,
      1.2795,
      3.9211,
      1.1456,
      3.6764,
      1.963,
      4.0431,
      4.4602,
      1.3398,
      1.0001,
      1.1018,
      1.5574,
      3.2471,
      1.0737,
      4.3384,
      2.7498,
      5.3927,
      2.9552,
      1.3104,
      1.0002,
      1.1227,
      1.3343,
      3.9649,
      1.1759,
      4.5983,
      3.4492,
      3.5275,
      2.0461,
      1.3369,
      1.0004,
      1.39,
      1.1756,
      3.9057,
      1.0006,
      2.1142,
      2.6632,
      7.2767,
      2.3723,
      1.3015,
      1.0006,
      1.1703,
      1.2453,
      4.6128,
      1.0005,
      1.7187,
      1.9028,
      6.0702,
      1.3133,
      1.3896,
      1.0778,
      1.4856,
      1.0003,
      1.1596,
      7.4874,
      6.1019,
      2.0208,
      4.5924,
      1.6168,
      10.5878,
      9.8288,
      6.1717,
      1.2082,
      2.7608,
      5.4764,
      5.5617,
      2.9664,
      5.4428,
      1.1624,
      1.5661,
      1.1076,
      1.4105,
      1.0011,
      1.0618,
      1.3272,
      2.1739,
      2.1401,
      4.4405,
      1.8454,
      1.4654,
      1.0003,
      1.2346,
      1.2885,
      4.7086,
      1.1822,
      4.3925,
      1.5913,
      4.4434,
      1.1305,
      1.7288,
      1.0383,
      1.3764,
      1.0001,
      1.1394,
      1.5509,
      3.8394
    ],
    "max": 10.5878,
    "min": 1.0,
    "mean": 2.2429
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8786,
    "per_token": [
      1.3322,
      1.96,
      0.1692,
      1.8239,
      1.8414,
      1.3361,
      0.0007,
      0.3992,
      1.1924,
      2.4518,
      0.6594,
      1.0679,
      0.0,
      0.0921,
      1.1483,
      1.9479,
      0.0256,
      1.1888,
      0.9399,
      0.0973,
      0.3441,
      0.1608,
      0.0,
      0.0042,
      0.312,
      1.3587,
      0.0225,
      0.702,
      1.0973,
      0.0627,
      0.3317,
      0.1534,
      0.0,
      0.0074,
      0.4373,
      0.56,
      0.0132,
      0.768,
      1.1756,
      0.0595,
      0.7279,
      3.1323,
      0.0,
      0.2455,
      0.6395,
      1.095,
      0.0072,
      0.0007,
      2.0133,
      0.3395,
      0.5757,
      1.9346,
      1.2964,
      0.381,
      0.0002,
      0.308,
      0.3093,
      1.8737,
      0.4942,
      1.9866,
      1.1602,
      1.9879,
      1.5145,
      0.4643,
      0.0002,
      0.2115,
      0.3479,
      2.0197,
      0.0002,
      0.5265,
      1.3479,
      2.416,
      2.0348,
      0.3235,
      0.0001,
      0.0719,
      0.3662,
      1.1634,
      0.0918,
      2.1439,
      0.8087,
      0.7785,
      0.0544,
      0.719,
      0.0563,
      0.335,
      0.0001,
      0.1195,
      0.4053,
      0.9563,
      1.0054,
      2.3142,
      0.1034,
      0.5734,
      0.068,
      0.7042,
      0.0001,
      0.0514,
      0.8646,
      0.9258,
      0.7795,
      2.4251,
      0.198,
      0.5047,
      0.094,
      0.711,
      0.0004,
      0.0871,
      0.9944,
      1.8083,
      0.8671,
      1.7901,
      1.2474,
      0.3892,
      0.0002,
      0.3244,
      0.3556,
      1.9712,
      0.1961,
      1.8783,
      0.9731,
      2.0154,
      2.1571,
      0.422,
      0.0001,
      0.1399,
      0.6392,
      1.6991,
      0.1026,
      2.1172,
      1.4593,
      2.431,
      1.5633,
      0.39,
      0.0003,
      0.167,
      0.4161,
      1.9873,
      0.2338,
      2.2011,
      1.7863,
      1.8186,
      1.0329,
      0.4189,
      0.0006,
      0.4751,
      0.2334,
      1.9656,
      0.0009,
      1.0801,
      1.4132,
      2.8633,
      1.2463,
      0.3802,
      0.0008,
      0.2269,
      0.3165,
      2.2056,
      0.0008,
      0.7813,
      0.9281,
      2.6018,
      0.3932,
      0.4747,
      0.1081,
      0.571,
      0.0004,
      0.2136,
      2.9045,
      2.6092,
      1.0149,
      2.1992,
      0.6931,
      3.4043,
      3.297,
      2.6257,
      0.2728,
      1.4651,
      2.4532,
      2.4755,
      1.5687,
      2.4444,
      0.2171,
      0.6472,
      0.1474,
      0.4962,
      0.0015,
      0.0865,
      0.4084,
      1.1203,
      1.0977,
      2.1507,
      0.8839,
      0.5513,
      0.0004,
      0.3041,
      0.3657,
      2.2353,
      0.2414,
      2.135,
      0.6702,
      2.1517,
      0.177,
      0.7898,
      0.0543,
      0.4609,
      0.0002,
      0.1883,
      0.6331,
      1.9409
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.1798,
    "entropy_zscore": 1.2064,
    "surprisal_zscore": 1.2064,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}