{
  "filename": "avantgarde_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:44:42.020850",
  "token_count": 50,
  "perplexity": {
    "overall": 1.6546,
    "per_token": [
      2.068,
      1.5283,
      1.3219,
      2.7627,
      2.3282,
      2.6111,
      1.0001,
      1.2839,
      2.3795,
      1.8819,
      1.7682,
      2.8119,
      1.138,
      1.5708,
      1.0831,
      1.7991,
      1.0005,
      1.181,
      1.2645,
      4.224,
      2.7938,
      4.8957,
      1.2872,
      1.0719,
      1.2939,
      1.303,
      2.6217,
      1.007,
      1.0767,
      2.1365,
      1.9617,
      3.3349,
      1.1033,
      1.4891,
      1.1298,
      1.3483,
      1.0002,
      1.0883,
      1.5544,
      2.824,
      2.0322,
      3.6614,
      1.1535,
      1.3817,
      1.1077,
      1.3149,
      1.0001,
      1.0712,
      1.5846,
      4.3971
    ],
    "max": 4.8957,
    "min": 1.0001,
    "mean": 1.8406
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7265,
    "per_token": [
      1.0482,
      0.6119,
      0.4026,
      1.4661,
      1.2192,
      1.3847,
      0.0001,
      0.3606,
      1.2506,
      0.9122,
      0.8222,
      1.4915,
      0.1865,
      0.6515,
      0.1152,
      0.8473,
      0.0007,
      0.24,
      0.3386,
      2.0786,
      1.4822,
      2.2915,
      0.3642,
      0.1002,
      0.3718,
      0.3818,
      1.3905,
      0.0101,
      0.1066,
      1.0953,
      0.9721,
      1.7377,
      0.1418,
      0.5745,
      0.176,
      0.4311,
      0.0003,
      0.122,
      0.6363,
      1.4977,
      1.0231,
      1.8724,
      0.206,
      0.4665,
      0.1476,
      0.3949,
      0.0001,
      0.0993,
      0.6641,
      2.1366
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.1886,
    "entropy_zscore": -0.1569,
    "surprisal_zscore": -0.1569,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}