{
  "filename": "traditional_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:43.428809",
  "token_count": 100,
  "perplexity": {
    "overall": 1.6515,
    "per_token": [
      2.3391,
      2.8098,
      1.0,
      1.0026,
      1.1892,
      2.5346,
      2.7751,
      1.0061,
      2.4229,
      2.2017,
      2.0893,
      2.6749,
      2.8975,
      1.0091,
      2.1757,
      3.3413,
      5.1483,
      1.011,
      3.4399,
      1.1995,
      2.5521,
      2.1746,
      1.4152,
      1.3942,
      1.1076,
      1.6278,
      1.0024,
      1.1303,
      1.2108,
      2.9462,
      3.3096,
      2.1034,
      1.463,
      1.4661,
      1.1667,
      1.9994,
      1.0074,
      1.1771,
      1.1195,
      2.9814,
      2.3088,
      2.9245,
      1.4539,
      1.3922,
      1.1403,
      1.5597,
      1.0029,
      1.2462,
      1.2561,
      3.2289,
      1.9244,
      2.3928,
      1.6205,
      1.408,
      1.1016,
      1.5008,
      1.0009,
      1.1184,
      1.2111,
      3.1417,
      1.4803,
      1.4518,
      1.0,
      1.0451,
      2.1301,
      1.9287,
      1.0369,
      1.2044,
      1.2474,
      1.4024,
      2.493,
      2.062,
      1.7577,
      1.5117,
      1.1078,
      1.6426,
      1.0033,
      1.158,
      1.1306,
      4.7058,
      2.1186,
      1.5449,
      1.8958,
      1.4281,
      1.1423,
      1.8428,
      1.0042,
      1.3069,
      1.1963,
      4.1578,
      2.2472,
      2.5287,
      1.7013,
      2.3517,
      1.0057,
      1.7692,
      1.1118,
      1.0003,
      4.5237,
      1.0592
    ],
    "max": 5.1483,
    "min": 1.0,
    "mean": 1.8199
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7238,
    "per_token": [
      1.2259,
      1.4905,
      0.0,
      0.0037,
      0.25,
      1.3417,
      1.4726,
      0.0088,
      1.2767,
      1.1386,
      1.063,
      1.4195,
      1.5348,
      0.0131,
      1.1214,
      1.7404,
      2.3641,
      0.0158,
      1.7824,
      0.2624,
      1.3517,
      1.1207,
      0.501,
      0.4794,
      0.1474,
      0.7029,
      0.0035,
      0.1767,
      0.276,
      1.5588,
      1.7267,
      1.0727,
      0.5489,
      0.5519,
      0.2224,
      0.9996,
      0.0106,
      0.2352,
      0.1629,
      1.576,
      1.2071,
      1.5482,
      0.5399,
      0.4773,
      0.1895,
      0.6413,
      0.0042,
      0.3175,
      0.329,
      1.691,
      0.9444,
      1.2587,
      0.6964,
      0.4937,
      0.1396,
      0.5857,
      0.0013,
      0.1615,
      0.2763,
      1.6515,
      0.5659,
      0.5378,
      0.0,
      0.0636,
      1.0909,
      0.9476,
      0.0523,
      0.2684,
      0.3189,
      0.4879,
      1.3179,
      1.044,
      0.8137,
      0.5962,
      0.1477,
      0.7159,
      0.0048,
      0.2117,
      0.1771,
      2.2344,
      1.0831,
      0.6275,
      0.9228,
      0.5141,
      0.192,
      0.8819,
      0.0061,
      0.3862,
      0.2586,
      2.0558,
      1.1681,
      1.3384,
      0.7666,
      1.2337,
      0.0083,
      0.8231,
      0.1529,
      0.0005,
      2.1775,
      0.083
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.2117,
    "entropy_zscore": -0.1811,
    "surprisal_zscore": -0.1811,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}