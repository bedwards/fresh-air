{
  "filename": "avantgarde_004.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:44:50.285064",
  "token_count": 50,
  "perplexity": {
    "overall": 1.7347,
    "per_token": [
      1.9904,
      1.5133,
      1.4351,
      2.9956,
      2.4332,
      2.2438,
      1.0001,
      1.3417,
      2.274,
      1.8718,
      2.0355,
      4.1429,
      1.1249,
      1.4284,
      1.0641,
      2.2436,
      1.0228,
      1.4744,
      1.3192,
      2.6674,
      2.2828,
      6.3006,
      2.4413,
      1.5262,
      1.0002,
      1.1748,
      1.2744,
      6.4032,
      1.0,
      1.4387,
      1.9644,
      4.7649,
      1.152,
      1.3795,
      1.0983,
      1.24,
      1.0,
      1.0189,
      1.9307,
      5.1043,
      1.8992,
      4.3857,
      1.1804,
      1.387,
      1.1278,
      1.326,
      1.0,
      1.0352,
      1.5033,
      2.7661
    ],
    "max": 6.4032,
    "min": 1.0,
    "mean": 2.0146
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7947,
    "per_token": [
      0.993,
      0.5977,
      0.5212,
      1.5829,
      1.2828,
      1.1659,
      0.0001,
      0.4241,
      1.1852,
      0.9044,
      1.0254,
      2.0506,
      0.1699,
      0.5144,
      0.0896,
      1.1658,
      0.0325,
      0.5601,
      0.3997,
      1.4154,
      1.1908,
      2.6555,
      1.2877,
      0.61,
      0.0003,
      0.2324,
      0.3498,
      2.6788,
      0.0001,
      0.5248,
      0.9741,
      2.2525,
      0.2042,
      0.4641,
      0.1352,
      0.3104,
      0.0,
      0.0271,
      0.9491,
      2.3517,
      0.9254,
      2.1328,
      0.2393,
      0.472,
      0.1735,
      0.4071,
      0.0001,
      0.0499,
      0.5882,
      1.4679
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.4071,
    "entropy_zscore": 0.4544,
    "surprisal_zscore": 0.4544,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}