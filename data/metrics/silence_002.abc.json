{
  "filename": "silence_002.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:05.705668",
  "token_count": 50,
  "perplexity": {
    "overall": 1.7798,
    "per_token": [
      2.3592,
      2.7989,
      1.0509,
      4.1584,
      3.0466,
      3.1257,
      1.0002,
      1.3901,
      2.3004,
      4.3778,
      2.2393,
      2.0779,
      1.0738,
      1.0152,
      1.3954,
      3.5395,
      1.3782,
      1.482,
      3.0166,
      2.3364,
      1.7399,
      1.6709,
      1.0075,
      1.8424,
      1.0277,
      1.1012,
      1.03,
      1.0,
      1.0,
      2.409,
      2.1279,
      3.1808,
      1.5233,
      2.3129,
      1.0812,
      1.8667,
      1.4578,
      4.8252,
      1.2816,
      2.0688,
      1.7301,
      2.1076,
      1.0,
      1.173,
      2.3534,
      3.0104,
      1.0065,
      1.257,
      3.1243,
      1.4591
    ],
    "max": 4.8252,
    "min": 1.0,
    "mean": 1.9788
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8317,
    "per_token": [
      1.2383,
      1.4849,
      0.0717,
      2.056,
      1.6072,
      1.6442,
      0.0004,
      0.4752,
      1.2019,
      2.1302,
      1.1631,
      1.0551,
      0.1027,
      0.0218,
      0.4807,
      1.8236,
      0.4628,
      0.5675,
      1.5929,
      1.2243,
      0.799,
      0.7406,
      0.0107,
      0.8816,
      0.0395,
      0.1391,
      0.0426,
      0.0001,
      0.0,
      1.2684,
      1.0894,
      1.6694,
      0.6072,
      1.2097,
      0.1127,
      0.9005,
      0.5438,
      2.2706,
      0.3579,
      1.0488,
      0.7909,
      1.0756,
      0.0,
      0.2302,
      1.2347,
      1.59,
      0.0094,
      0.3299,
      1.6436,
      0.5451
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.7425,
    "entropy_zscore": 0.7861,
    "surprisal_zscore": 0.7861,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}