{
  "filename": "avantgarde_003.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:44:47.439930",
  "token_count": 96,
  "perplexity": {
    "overall": 1.6361,
    "per_token": [
      2.0896,
      1.6736,
      1.2973,
      2.1707,
      2.2874,
      1.8676,
      1.0001,
      1.2198,
      2.5331,
      1.496,
      1.8178,
      2.7654,
      1.184,
      1.5963,
      1.0805,
      1.9759,
      1.0011,
      1.2198,
      1.2322,
      3.4295,
      2.0827,
      6.5862,
      1.0675,
      1.5845,
      1.1442,
      1.23,
      1.0,
      1.0467,
      2.2561,
      5.1329,
      1.6141,
      3.2343,
      1.0442,
      1.4684,
      1.0609,
      1.4484,
      1.0001,
      1.0328,
      2.489,
      2.1024,
      1.7386,
      5.0827,
      1.0885,
      1.4327,
      1.044,
      1.517,
      1.0001,
      1.0624,
      1.3495,
      3.2082,
      1.5987,
      3.5033,
      1.1271,
      1.7607,
      1.0803,
      1.251,
      1.0,
      1.0832,
      8.8138,
      1.9055,
      2.3971,
      3.0273,
      1.0717,
      1.7055,
      1.0959,
      1.2608,
      1.0001,
      1.0863,
      1.4751,
      3.6533,
      1.6358,
      3.1063,
      1.082,
      1.6814,
      1.0899,
      1.2445,
      1.0001,
      1.0656,
      7.0799,
      2.2413,
      2.3287,
      1.0552,
      1.0,
      1.0022,
      1.9389,
      2.0259,
      1.1365,
      3.1035,
      1.0547,
      1.3362,
      2.1365,
      4.201,
      1.1515,
      1.3861,
      1.0797,
      1.351
    ],
    "max": 8.8138,
    "min": 1.0,
    "mean": 1.901
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.7103,
    "per_token": [
      1.0632,
      0.7429,
      0.3755,
      1.1182,
      1.1937,
      0.9012,
      0.0001,
      0.2867,
      1.3409,
      0.5811,
      0.8622,
      1.4675,
      0.2437,
      0.6747,
      0.1117,
      0.9825,
      0.0015,
      0.2866,
      0.3012,
      1.778,
      1.0584,
      2.7194,
      0.0942,
      0.664,
      0.1943,
      0.2987,
      0.0,
      0.0659,
      1.1738,
      2.3598,
      0.6908,
      1.6934,
      0.0624,
      0.5542,
      0.0853,
      0.5344,
      0.0001,
      0.0466,
      1.3156,
      1.072,
      0.7979,
      2.3456,
      0.1223,
      0.5188,
      0.0621,
      0.6012,
      0.0002,
      0.0874,
      0.4325,
      1.6817,
      0.6769,
      1.8087,
      0.1726,
      0.8162,
      0.1115,
      0.3231,
      0.0001,
      0.1153,
      3.1398,
      0.9302,
      1.2613,
      1.598,
      0.0999,
      0.7702,
      0.1321,
      0.3344,
      0.0001,
      0.1195,
      0.5609,
      1.8692,
      0.71,
      1.6352,
      0.1136,
      0.7497,
      0.1242,
      0.3156,
      0.0002,
      0.0917,
      2.8237,
      1.1644,
      1.2195,
      0.0775,
      0.0,
      0.0032,
      0.9552,
      1.0186,
      0.1846,
      1.6339,
      0.0769,
      0.4182,
      1.0953,
      2.0707,
      0.2035,
      0.471,
      0.1106,
      0.4341
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": -0.3262,
    "entropy_zscore": -0.3021,
    "surprisal_zscore": -0.3021,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}