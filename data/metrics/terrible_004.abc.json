{
  "filename": "terrible_004.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:30.028701",
  "token_count": 60,
  "perplexity": {
    "overall": 1.8338,
    "per_token": [
      3.3698,
      1.5752,
      1.1091,
      1.0284,
      1.3918,
      1.6024,
      2.2579,
      4.3727,
      2.0421,
      1.0018,
      2.0702,
      2.0552,
      1.0351,
      1.0023,
      1.2418,
      3.4541,
      1.2823,
      2.5763,
      2.7319,
      3.0362,
      2.2218,
      3.7265,
      1.3033,
      2.4451,
      2.8585,
      1.2484,
      4.1625,
      1.1191,
      1.0058,
      1.0054,
      3.4261,
      5.6764,
      2.2606,
      4.7622,
      1.7949,
      2.3042,
      1.1158,
      1.8424,
      1.9019,
      1.2523,
      2.6243,
      4.1211,
      1.5607,
      2.0528,
      2.0373,
      2.4911,
      1.9512,
      1.253,
      1.0104,
      1.0204,
      1.4975,
      1.2523,
      1.0,
      1.0466,
      1.4886,
      1.5907,
      1.0102,
      1.9297,
      3.878,
      1.5251
    ],
    "max": 5.6764,
    "min": 1.0,
    "mean": 2.0668
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8748,
    "per_token": [
      1.7527,
      0.6555,
      0.1494,
      0.0404,
      0.477,
      0.6802,
      1.175,
      2.1285,
      1.03,
      0.0025,
      1.0498,
      1.0393,
      0.0498,
      0.0034,
      0.3124,
      1.7883,
      0.3587,
      1.3653,
      1.4499,
      1.6022,
      1.1517,
      1.8978,
      0.3822,
      1.2899,
      1.5153,
      0.3201,
      2.0575,
      0.1623,
      0.0084,
      0.0077,
      1.7766,
      2.505,
      1.1767,
      2.2516,
      0.8439,
      1.2042,
      0.1581,
      0.8816,
      0.9274,
      0.3246,
      1.3919,
      2.043,
      0.6422,
      1.0376,
      1.0266,
      1.3168,
      0.9643,
      0.3254,
      0.015,
      0.0292,
      0.5826,
      0.3246,
      0.0,
      0.0657,
      0.5739,
      0.6696,
      0.0146,
      0.9484,
      1.9553,
      0.6089
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.1441,
    "entropy_zscore": 1.1724,
    "surprisal_zscore": 1.1724,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}