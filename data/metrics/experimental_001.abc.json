{
  "filename": "experimental_001.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:45:12.222338",
  "token_count": 290,
  "perplexity": {
    "overall": 1.8256,
    "per_token": [
      2.5578,
      2.8295,
      1.7334,
      4.3157,
      1.794,
      4.6159,
      3.3264,
      2.9117,
      1.0244,
      3.0502,
      1.6802,
      2.1163,
      1.0,
      1.0687,
      2.2092,
      3.677,
      1.0188,
      2.3557,
      2.0108,
      1.0688,
      1.2693,
      1.1179,
      1.0,
      1.0029,
      1.2414,
      2.5645,
      1.0157,
      1.6267,
      2.1396,
      1.0444,
      1.2585,
      1.1122,
      1.0,
      1.0051,
      1.3541,
      1.4743,
      1.0092,
      1.7029,
      2.2589,
      1.0421,
      1.7385,
      7.5773,
      1.1785,
      12.4465,
      2.1185,
      2.2991,
      2.5011,
      1.0087,
      1.0,
      1.0,
      1.8663,
      4.1397,
      4.7867,
      1.368,
      1.0001,
      1.1154,
      1.3521,
      2.687,
      1.0749,
      4.3894,
      1.9748,
      5.0934,
      1.4563,
      6.9073,
      1.8531,
      7.7865,
      2.3068,
      1.1122,
      1.0755,
      2.8136,
      2.3101,
      4.3605,
      1.3751,
      9.2822,
      1.0184,
      5.1844,
      1.0012,
      1.8698,
      1.1199,
      1.261,
      2.0155,
      6.1728,
      2.7212,
      1.3109,
      1.0001,
      1.1036,
      1.3485,
      3.5033,
      1.1402,
      3.6956,
      2.3016,
      3.3894,
      3.2105,
      1.2695,
      1.0001,
      1.0973,
      9.9693,
      1.043,
      2.3343,
      1.0045,
      1.8662,
      5.2534,
      3.6536,
      1.1628,
      1.0001,
      1.1199,
      1.3015,
      2.1152,
      1.0831,
      5.5812,
      1.7476,
      4.6509,
      1.2913,
      1.4208,
      1.0615,
      1.3972,
      1.0005,
      1.2219,
      1.6216,
      2.6886,
      2.2811,
      5.4999,
      2.1819,
      1.3879,
      1.0002,
      1.1966,
      1.2262,
      5.6237,
      1.1269,
      5.1346,
      1.7775,
      4.6984,
      1.0635,
      1.4204,
      1.079,
      1.5422,
      1.0002,
      1.0645,
      1.664,
      2.5676,
      2.4595,
      1.3136,
      1.0,
      1.0381,
      1.9945,
      2.0257,
      1.0053,
      1.7433,
      1.9205,
      1.4961,
      2.3807,
      3.4359,
      2.6218,
      1.3822,
      1.0002,
      1.18,
      1.2602,
      4.9568,
      1.1235,
      4.5355,
      1.7076,
      2.8361,
      1.0689,
      1.7301,
      1.0751,
      1.354,
      1.0001,
      1.093,
      1.3956,
      2.291,
      2.242,
      4.8093,
      2.4613,
      1.2752,
      1.0002,
      1.2038,
      1.2871,
      3.7478,
      1.1496,
      4.2012,
      1.7408,
      5.3618,
      2.8732,
      1.2812,
      1.0003,
      1.2561,
      1.2514,
      5.6866,
      1.1121,
      3.2691,
      2.1992,
      3.6605,
      2.1405,
      1.3332,
      1.0004,
      1.2813,
      1.214,
      3.4707,
      1.0004,
      1.3314,
      1.9215,
      3.4971,
      2.0493,
      1.2942,
      1.0001,
      1.1671,
      1.2002,
      4.3686,
      1.2631,
      4.3685,
      1.9341,
      1.8675,
      1.0522,
      1.5971,
      1.0864,
      1.3805,
      1.0002,
      1.0989,
      1.7186,
      3.3051,
      2.4495,
      5.3268,
      1.6385,
      7.1083,
      1.6533,
      9.07,
      4.8905,
      1.6341,
      1.5535,
      1.374,
      2.322,
      1.5681,
      1.0,
      1.1091,
      1.7694,
      3.3659,
      1.2386,
      1.5892,
      1.1816,
      1.2128,
      1.8862,
      4.7601,
      2.3558,
      1.3118,
      1.0001,
      1.0891,
      1.4175,
      4.096,
      1.0004,
      1.5025,
      2.1925,
      2.4265,
      1.2488,
      1.462,
      1.1027,
      1.5243,
      1.0008,
      1.2096,
      1.4297,
      6.0629,
      2.3301,
      5.0484,
      1.2578,
      1.4419,
      1.1364,
      2.1941,
      1.0487,
      2.2548,
      1.7573,
      5.4173,
      2.7261,
      4.5942,
      1.084,
      1.5701,
      1.2229,
      1.3892,
      1.0002,
      1.0694,
      1.7459,
      3.9867,
      2.1781,
      1.6264,
      1.0,
      1.1192,
      1.7274,
      1.8135,
      1.0759,
      1.5163,
      1.3182,
      1.2001
    ],
    "max": 12.4465,
    "min": 1.0,
    "mean": 2.2112
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.8684,
    "per_token": [
      1.3549,
      1.5005,
      0.7936,
      2.1096,
      0.8432,
      2.2066,
      1.734,
      1.5419,
      0.0348,
      1.6089,
      0.7486,
      1.0815,
      0.0,
      0.0959,
      1.1435,
      1.8785,
      0.0268,
      1.2361,
      1.0078,
      0.096,
      0.3441,
      0.1608,
      0.0,
      0.0042,
      0.312,
      1.3587,
      0.0225,
      0.702,
      1.0973,
      0.0627,
      0.3317,
      0.1534,
      0.0,
      0.0074,
      0.4373,
      0.56,
      0.0132,
      0.768,
      1.1756,
      0.0595,
      0.7978,
      2.9217,
      0.237,
      3.6377,
      1.0831,
      1.2011,
      1.3226,
      0.0126,
      0.0,
      0.0,
      0.9002,
      2.0495,
      2.259,
      0.4521,
      0.0001,
      0.1575,
      0.4352,
      1.426,
      0.1042,
      2.134,
      0.9817,
      2.3486,
      0.5423,
      2.7881,
      0.89,
      2.961,
      1.2059,
      0.1534,
      0.105,
      1.4924,
      1.2079,
      2.1245,
      0.4596,
      3.2145,
      0.0263,
      2.3742,
      0.0018,
      0.9029,
      0.1634,
      0.3346,
      1.0111,
      2.6259,
      1.4442,
      0.3906,
      0.0002,
      0.1422,
      0.4314,
      1.8087,
      0.1893,
      1.8858,
      1.2027,
      1.761,
      1.6828,
      0.3442,
      0.0001,
      0.134,
      3.3175,
      0.0607,
      1.223,
      0.0065,
      0.9001,
      2.3932,
      1.8693,
      0.2176,
      0.0001,
      0.1633,
      0.3802,
      1.0808,
      0.1152,
      2.4806,
      0.8054,
      2.2175,
      0.3689,
      0.5067,
      0.086,
      0.4825,
      0.0007,
      0.2891,
      0.6974,
      1.4268,
      1.1897,
      2.4594,
      1.1256,
      0.4729,
      0.0003,
      0.2589,
      0.2942,
      2.4915,
      0.1724,
      2.3602,
      0.8298,
      2.2322,
      0.0888,
      0.5063,
      0.1097,
      0.625,
      0.0003,
      0.0902,
      0.7346,
      1.3604,
      1.2983,
      0.3935,
      0.0,
      0.054,
      0.9961,
      1.0184,
      0.0076,
      0.8018,
      0.9415,
      0.5812,
      1.2514,
      1.7807,
      1.3906,
      0.467,
      0.0002,
      0.2388,
      0.3336,
      2.3094,
      0.168,
      2.1813,
      0.772,
      1.5039,
      0.0961,
      0.7909,
      0.1044,
      0.4372,
      0.0002,
      0.1283,
      0.4809,
      1.196,
      1.1648,
      2.2658,
      1.2994,
      0.3507,
      0.0003,
      0.2676,
      0.3641,
      1.9061,
      0.2012,
      2.0708,
      0.7998,
      2.4227,
      1.5226,
      0.3575,
      0.0004,
      0.329,
      0.3235,
      2.5076,
      0.1533,
      1.7089,
      1.137,
      1.872,
      1.0979,
      0.4149,
      0.0006,
      0.3576,
      0.2798,
      1.7952,
      0.0005,
      0.413,
      0.9422,
      1.8062,
      1.0352,
      0.3721,
      0.0002,
      0.2229,
      0.2632,
      2.1272,
      0.337,
      2.1271,
      0.9517,
      0.9011,
      0.0734,
      0.6754,
      0.1195,
      0.4652,
      0.0002,
      0.1361,
      0.7812,
      1.7247,
      1.2925,
      2.4133,
      0.7123,
      2.8295,
      0.7254,
      3.1811,
      2.29,
      0.7085,
      0.6356,
      0.4584,
      1.2154,
      0.649,
      0.0,
      0.1494,
      0.8233,
      1.751,
      0.3087,
      0.6683,
      0.2408,
      0.2783,
      0.9155,
      2.251,
      1.2362,
      0.3915,
      0.0001,
      0.1231,
      0.5033,
      2.0342,
      0.0006,
      0.5874,
      1.1326,
      1.2789,
      0.3206,
      0.548,
      0.1411,
      0.6082,
      0.0011,
      0.2745,
      0.5157,
      2.6,
      1.2204,
      2.3358,
      0.3309,
      0.5279,
      0.1844,
      1.1336,
      0.0686,
      1.173,
      0.8134,
      2.4376,
      1.4468,
      2.1998,
      0.1164,
      0.6509,
      0.2903,
      0.4743,
      0.0003,
      0.0968,
      0.804,
      1.9952,
      1.123,
      0.7017,
      0.0,
      0.1625,
      0.7886,
      0.8588,
      0.1056,
      0.6006,
      0.3986,
      0.2632
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 1.0831,
    "entropy_zscore": 1.115,
    "surprisal_zscore": 1.115,
    "outlier_flags": [],
    "classification": "syntactically_valid_semantically_coherent",
    "interpretation": "The model processes this piece comfortably, recognizing familiar patterns. Both token sequences and structural relationships align with training data."
  }
}