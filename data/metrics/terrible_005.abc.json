{
  "filename": "terrible_005.abc",
  "model": "phi4:14b",
  "timestamp": "2026-01-21T09:47:33.904820",
  "token_count": 70,
  "perplexity": {
    "overall": 1.7088,
    "per_token": [
      2.8604,
      1.529,
      1.1028,
      1.0274,
      1.3957,
      1.5897,
      2.3549,
      4.4023,
      3.1397,
      2.7035,
      2.0702,
      2.0552,
      1.0351,
      1.0023,
      1.2418,
      3.4541,
      1.2823,
      2.5763,
      2.7319,
      3.0362,
      1.8373,
      3.7594,
      5.5182,
      1.6709,
      1.0001,
      1.4802,
      2.0878,
      4.4049,
      1.0559,
      1.1306,
      2.1423,
      1.8279,
      1.5535,
      1.4037,
      1.0384,
      1.5948,
      1.0001,
      1.0801,
      1.08,
      1.1608,
      1.2335,
      9.8064,
      1.4925,
      2.5074,
      1.8261,
      1.7507,
      1.7356,
      1.1571,
      1.007,
      1.0185,
      2.4164,
      1.4975,
      1.3558,
      1.4499,
      1.0532,
      1.3357,
      1.0001,
      1.053,
      1.0985,
      1.2239,
      1.577,
      3.3749,
      1.7713,
      1.0922,
      1.4567,
      1.5995,
      3.569,
      2.5886,
      1.8768,
      1.1133
    ],
    "max": 9.8064,
    "min": 1.0001,
    "mean": 1.9637
  },
  "attention": {
    "mean_entropy": null,
    "per_layer": null,
    "per_head": null,
    "high_entropy_heads": [],
    "note": "Attention weights not available via Ollama API. Use transformers library with output_attentions=True for direct model access to attention patterns."
  },
  "surprisal": {
    "mean": 0.773,
    "per_token": [
      1.5162,
      0.6126,
      0.1411,
      0.0389,
      0.481,
      0.6687,
      1.2357,
      2.1383,
      1.6506,
      1.4348,
      1.0498,
      1.0393,
      0.0498,
      0.0034,
      0.3124,
      1.7883,
      0.3587,
      1.3653,
      1.4499,
      1.6022,
      0.8776,
      1.9105,
      2.4642,
      0.7406,
      0.0001,
      0.5658,
      1.062,
      2.1391,
      0.0785,
      0.1771,
      1.0991,
      0.8702,
      0.6355,
      0.4892,
      0.0543,
      0.6733,
      0.0002,
      0.1111,
      0.111,
      0.2151,
      0.3028,
      3.2937,
      0.5778,
      1.3262,
      0.8687,
      0.8079,
      0.7954,
      0.2105,
      0.0101,
      0.0264,
      1.2728,
      0.5826,
      0.4391,
      0.5359,
      0.0748,
      0.4176,
      0.0001,
      0.0745,
      0.1355,
      0.2914,
      0.6572,
      1.7549,
      0.8248,
      0.1272,
      0.5427,
      0.6776,
      1.8355,
      1.3722,
      0.9083,
      0.1548
    ],
    "high_surprisal_tokens": []
  },
  "activations": {
    "per_layer_norm": null,
    "variance_per_layer": null,
    "note": "Internal activations not available via Ollama API. Use transformers library with output_hidden_states=True for direct model access to layer activations."
  },
  "comparative": {
    "baseline": "traditional_mean",
    "perplexity_zscore": 0.2145,
    "entropy_zscore": 0.2599,
    "surprisal_zscore": 0.2599,
    "outlier_flags": [],
    "classification": "statistically_normal",
    "interpretation": "This piece falls well within the normal range of the model's experience with musical notation. All metrics indicate familiar, predictable patterns."
  }
}